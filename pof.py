# -*- coding: utf-8 -*-
"""
PoF - Clean Production Pipeline | Temporal Validation + Equipment Stratification
==================================================================================
Single script: Data Loading â†’ Feature Engineering â†’ Survival Models â†’ Risk Assessment
"""

import os
import sys
import json
import logging
from datetime import datetime
from typing import Tuple
from tqdm import tqdm  # <--- Add this with other imports
import numpy as np
import pandas as pd
import yaml
from scipy import stats
from sklearn.feature_selection import VarianceThreshold
from joblib import Parallel, delayed
# Soft dependencies
try:
    from lifelines import CoxPHFitter, WeibullAFTFitter
    from lifelines.utils import concordance_index
    LIFELINES_OK = True
except ImportError:
    LIFELINES_OK = False

try:
    from sksurv.ensemble import RandomSurvivalForest
    from sksurv.util import Surv
    from sksurv.metrics import concordance_index_censored
    SKSURV_OK = True
except ImportError:
    SKSURV_OK = False

try:
    from xgboost import XGBClassifier
    XGB_OK = True
except ImportError:
    XGB_OK = False

try:
    from catboost import CatBoostClassifier
    CAT_OK = True
except ImportError:
    CAT_OK = False

from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

# =============================================================================
# CONFIGURATION
# =============================================================================
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

with open(os.path.join(BASE_DIR, "config.yaml"), "r", encoding="utf-8") as f:
    CFG = yaml.safe_load(f)

DATA_DIR = os.path.join(BASE_DIR, CFG["paths"]["data"]["base"])
INPUT_DIR = os.path.join(BASE_DIR, CFG["paths"]["data"]["input"])
INTERMEDIATE_DIR = os.path.join(BASE_DIR, CFG["paths"]["data"]["intermediate"])
OUTPUT_DIR = os.path.join(BASE_DIR, CFG["paths"]["data"]["output"])
LOG_DIR = os.path.join(BASE_DIR, CFG["paths"]["data"]["logs"])

DATA_PATHS = {k: os.path.join(BASE_DIR, v) for k, v in CFG["data_paths"].items()}
INTERMEDIATE_PATHS = {k: os.path.join(INTERMEDIATE_DIR, v) for k, v in CFG["intermediate_paths"].items()}
OUTPUT_PATHS = {k: os.path.join(OUTPUT_DIR, v) for k, v in CFG["output_paths"].items()}
OBSERVATION_START_DATE = pd.Timestamp("2021-01-01")
SURVIVAL_HORIZONS_DAYS = CFG["survival"]["horizons_days"]
SURVIVAL_HORIZON_LABELS = CFG["survival"]["horizon_labels"]
MIN_EQUIPMENT_PER_CLASS = CFG["analysis"]["min_equipment_per_class"]
ANALYSIS_METADATA_PATH = os.path.join(BASE_DIR, CFG["analysis"]["analysis_metadata_path"])

CHRONIC_CFG = CFG.get("chronic", {})
CHRONIC_WINDOW_DAYS = CHRONIC_CFG.get("window_days_default", 90)
CHRONIC_THRESHOLD_EVENTS = CHRONIC_CFG.get("min_events_default", 3)
CHRONIC_MIN_RATE = CHRONIC_CFG.get("min_rate_per_year_default", 1.5)

# Data Balancing Configuration
BALANCE_CFG = CFG.get("data_balancing", {})
BALANCE_ENABLED = BALANCE_CFG.get("enabled", True)
BALANCE_TARGET_RATIO = BALANCE_CFG.get("target_ratio", 5)  # 1:5 default
BALANCE_METHOD = BALANCE_CFG.get("method", "undersample")
BALANCE_RANDOM_STATE = BALANCE_CFG.get("random_state", 42)

# =============================================================================
# ğŸ§  FEATURE REGISTRY (Ã–ZELLÄ°K YÃ–NETÄ°M MERKEZÄ°)
# =============================================================================
# Bu yapÄ±, modelin eÄŸitim stratejisini belirleyen merkezi konfigÃ¼rasyondur.
# Modelin "Neyi Ã¶ÄŸrenmesi gerektiÄŸi" (X) ve "Neyi gÃ¶rmemesi gerektiÄŸi" (Leakage) burada tanÄ±mlanÄ±r.
#
# ğŸš« 1. temporal_leakage (YASAKLI LÄ°STE / DATA LEAKAGE):
#    - Bu deÄŸiÅŸkenler, modelin tahmin etmeye Ã§alÄ±ÅŸtÄ±ÄŸÄ± "hedefi" (Target) veya 
#      henÃ¼z gerÃ§ekleÅŸmemiÅŸ "gelecek bilgisini" iÃ§erir.
#    - Ã–rn: 'event' (sonuÃ§), 'duration_days' (Ã¶mÃ¼r), 'Son_Ariza_Tarihi'.
#    - KRÄ°TÄ°K: Bu deÄŸiÅŸkenler eÄŸitim matrisinden (X) kesinlikle Ã‡IKARILIR.
#
# ğŸ“‰ 2. chronic_features (DÄ°NAMÄ°K SAÄLIK GÃ–STERGELERÄ°):
#    - VarlÄ±ÄŸÄ±n geÃ§miÅŸ performansÄ±ndan tÃ¼retilen matematiksel Ã¶zelliklerdir.
#    - IEEE 1366 standartlarÄ±na gÃ¶re kroniklik durumu (Flag), arÄ±za sÄ±klÄ±ÄŸÄ± (Rate) 
#      ve zaman aÄŸÄ±rlÄ±klÄ± yÄ±pranma skorunu (Decay) iÃ§erir.
#    - Modelin varlÄ±ÄŸÄ± "riskli" olarak tanÄ±masÄ±nÄ± saÄŸlayan ana sinyallerdir.
#
# ğŸ—ï¸ 3. structural_features (STATÄ°K YAPISAL Ã–ZELLÄ°KLER):
#    - VarlÄ±ÄŸÄ±n kimliÄŸi, fiziksel Ã¶zellikleri ve coÄŸrafi konumudur.
#    - Marka, Tip, Gerilim Seviyesi, Ä°lÃ§e gibi genelde sabit kalan niteliklerdir.
#    - Modelin "Hangi marka/tip daha dayanÄ±ksÄ±z?" sorusunu Ã§Ã¶zmesini saÄŸlar.
# =============================================================================
FEATURE_REGISTRY = {
    "temporal_leakage": ["event", "duration_days", "Ilk_Ariza_Tarihi", "Son_Ariza_Tarihi", 
                        "Fault_Count", "Ariza_Gecmisi"],
    "chronic_features": ["Chronic_Flag", "Chronic_Decay_Skoru", "MTBF_Bayes_Gun", 
                        "Chronic_Trend_Slope", "Chronic_Rate_Yillik"],
    "structural_features": ["cbs_id", "Ekipman_Tipi", "Kurulum_Tarihi", "Gerilim_Sinifi", 
                           "Gerilim_Seviyesi", "Marka", "kVA_Rating", "Sehir", "Ilce", 
                           "Mahalle", "Location_Known", "Musteri_Sayisi"],
}

# =============================================================================
# NOTE: REAL_FAILURE_CODES filter removed
# Domain expert confirmed all cause codes in the input data are valid failures
# Data quality checks (null cbs_id, etc.) are performed in load_fault_data()
# =============================================================================

# =============================================================================
# LOGGING
# =============================================================================
def setup_logger() -> logging.Logger:
    os.makedirs(LOG_DIR, exist_ok=True)
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_path = os.path.join(LOG_DIR, f"pof_{ts}.log")

    logger = logging.getLogger("pof")
    logger.setLevel(logging.INFO)
    logger.handlers.clear()

    fh = logging.FileHandler(log_path, encoding="utf-8")
    fh.setFormatter(logging.Formatter("%(asctime)s - %(levelname)s - %(message)s"))
    logger.addHandler(fh)

    import io
    ch = logging.StreamHandler(io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace'))
    ch.setFormatter(logging.Formatter("%(message)s"))
    logger.addHandler(ch)

    logger.info("="*80)
    logger.info("PoF Pipeline - Clean Production Version")
    logger.info("="*80)
    return logger

# =============================================================================
# UTILITIES
# =============================================================================
def ensure_dirs():
    for d in [INTERMEDIATE_DIR, OUTPUT_DIR, LOG_DIR]:
        os.makedirs(d, exist_ok=True)
    for p in list(INTERMEDIATE_PATHS.values()) + list(OUTPUT_PATHS.values()):
        os.makedirs(os.path.dirname(p), exist_ok=True)

def parse_date_safely(x):
    """
    Karma tarih formatlarÄ±nÄ± gÃ¼venli ÅŸekilde parse eder.
    Desteklenen formatlar:
    - Excel seri numarasÄ± (44567.5)
    - DD-MM-YYYY / DD.MM.YYYY (TR format)
    - YYYY-MM-DD (ISO format)
    - Datetime objesi (zaten iÅŸlenmiÅŸ)
    """
    if pd.isna(x) or str(x).strip() == "":
        return pd.NaT

    try:
        # EÄŸer veri zaten datetime objesi ise (Excel okurken bazen otomatik Ã§evirir)
        if isinstance(x, (pd.Timestamp, datetime)):
            return x

        # EÄŸer veri Excel seri numarasÄ± (float/int) olarak geldiyse (Ã–rn: 44567.5)
        if isinstance(x, (int, float)):
            # Excel baÅŸlangÄ±Ã§ tarihi: 30 AralÄ±k 1899
            return pd.to_datetime(x, unit='D', origin='1899-12-30')

        # String formatlarÄ± (Karma format desteÄŸi)
        date_str = str(x).strip()

        # Deneme sÄ±rasÄ± (TR formatÄ± Ã¶ncelikli)
        formats = [
            '%d-%m-%Y %H:%M:%S',  # 22-06-2025 04:59:21
            '%d-%m-%Y',           # 05-03-2025
            '%d.%m.%Y %H:%M:%S',  # 22.06.2025 04:59:21
            '%d.%m.%Y',           # 22.06.2025
            '%Y-%m-%d %H:%M:%S',  # 2023-01-17 17:14:42
            '%Y-%m-%d',           # 2023-01-17
        ]

        for fmt in formats:
            try:
                return pd.to_datetime(date_str, format=fmt)
            except:
                continue

        # HiÃ§biri iÅŸe yaramazsa pandas otomatik (dayfirst=True)
        return pd.to_datetime(x, errors="coerce", dayfirst=True)

    except Exception:
        return pd.NaT

def clean_equipment_type(series: pd.Series) -> pd.Series:
    return (series.astype(str).str.strip()
            .str.replace(" ArÄ±zalarÄ±", "", regex=False)
            .str.replace(" Ariza", "", regex=False)
            .str.strip())

def convert_duration_minutes(series: pd.Series, logger: logging.Logger) -> pd.Series:
    s = pd.to_numeric(series, errors="coerce")
    med = s.median()
    if pd.notna(med) and med > 10000:
        logger.info("[DURATION] Converting from milliseconds to minutes")
        return s / 60000.0
    return s
# =============================================================================
# â³ TEMPORAL SPLIT (ZAMANSAL BÃ–LÃœNME & SIZINTI Ã–NLEME)
# =============================================================================
# Bu fonksiyon, modelin "geleceÄŸi gÃ¶rmesini" (Data Leakage) engelleyen en kritik gÃ¼venlik duvarÄ±dÄ±r.
#
# ğŸš« Neden Rastgele (Random) BÃ¶lmÃ¼yoruz?
#    - Rastgele bÃ¶lme yaparsak, 2024 yÄ±lÄ±ndaki bir arÄ±zayÄ± eÄŸitim setine, 
#      2020 yÄ±lÄ±ndaki saÄŸlam durumu test setine koyabiliriz.
#    - Bu durumda model, "gelecekteki bilgiyi" kullanarak "geÃ§miÅŸi" tahmin eder.
#    - SonuÃ§: Test baÅŸarÄ±sÄ± yapay olarak yÃ¼ksek Ã§Ä±kar (%99) ama canlÄ±da baÅŸarÄ±sÄ±z olur.
#
# âœ… NasÄ±l Ã‡alÄ±ÅŸÄ±r?
#    1. TÃ¼m varlÄ±klarÄ± KURULUM TARÄ°HÄ°NE gÃ¶re eskiden yeniye sÄ±ralar.
#    2. Zaman Ã§izgisinin %75'inde bir kesme noktasÄ± (Cutoff) belirler.
#    3. GeÃ§miÅŸ %75 -> EÄÄ°TÄ°M SETÄ° (Model sadece geÃ§miÅŸi bilir).
#    4. Gelecek %25 -> TEST SETÄ° (Modelin hiÃ§ gÃ¶rmediÄŸi gelecek).
#
# ğŸ§  Teknik Detay:
#    - Fonksiyon, veri setini kopyalamak yerine, orijinal DataFrame'in 
#      Ä°NDEKS ETÄ°KETLERÄ°NÄ° (Index Labels) dÃ¶ndÃ¼rÃ¼r.
#    - Bu yÃ¶ntem, pandas sÄ±ralama iÅŸlemlerinde kayan indeks hatalarÄ±nÄ± (Loc vs Iloc) Ã¶nler.

# ğŸŒŸ AKILLI KESME (SMART CUTOFF):
#    - Standart bÃ¶lme sonucu Test setinde yeterli arÄ±za (Event) kalmazsa,
#      fonksiyon otomatik olarak kesme noktasÄ±nÄ± 6 ay geriye Ã§eker.
#    - AmaÃ§: Test setinin "tamamen saÄŸlÄ±klÄ±" varlÄ±klardan oluÅŸmasÄ±nÄ± engelleyip
#      AUC skorunun hesaplanabilir olmasÄ±nÄ± saÄŸlamaktÄ±r.
# =============================================================================
# =============================================================================
# TEMPORAL SPLIT (Core of Leakage Prevention)
# =============================================================================
def temporal_train_test_split(
    df: pd.DataFrame,
    test_size: float = 0.25,
    min_test_event_rate: float = 0.08,  # âœ… DÃœÅÃœRÃœLDÃœ: %8 (daha esnek)
    use_stratified_fallback: bool = False,  # âœ… KAPATILDI: Temporal dÃ¼zeltildi
    apply_balancing: bool = True,  # âœ… NEW: Balance train/test separately AFTER split
    logger: logging.Logger = None
) -> Tuple[np.ndarray, np.ndarray]:
    """
    âœ… FIXED: Event-aware temporal split with POST-SPLIT balancing

    Args:
        df: DataFrame with 'Kurulum_Tarihi' and 'event' columns
        test_size: Fraction of data for test set
        min_test_event_rate: Minimum acceptable event rate in test set (default 8%)
        use_stratified_fallback: DEPRECATED - kept for compatibility
        logger: Logger instance

    Returns:
        train_labels, test_labels (index arrays)
    """
    
    if "Kurulum_Tarihi" not in df.columns:
        raise ValueError("Temporal split requires 'Kurulum_Tarihi' column")
    
    install_dates = pd.to_datetime(df["Kurulum_Tarihi"], errors="coerce")
    if install_dates.isna().all():
        raise ValueError("All Kurulum_Tarihi values are invalid")
    
    df_sorted = df.copy()
    df_sorted["_install_clean"] = install_dates
    df_sorted = df_sorted.sort_values("_install_clean")
    
    # Standard cutoff
    cutoff_pos_initial = int(len(df_sorted) * (1 - test_size))
    
    # Check test event rate
    if "event" in df.columns:
        test_events_initial = df_sorted.iloc[cutoff_pos_initial:]["event"].mean()
        
        # If too low, adjust cutoff backwards
        if test_events_initial < min_test_event_rate:
            if logger:
                logger.warning(f"[SPLIT] Initial test events: {test_events_initial:.1%} < {min_test_event_rate:.1%}")

            # Move cutoff back by 6 months
            cutoff_date_initial = df_sorted.iloc[cutoff_pos_initial]["_install_clean"]
            cutoff_date_adjusted = cutoff_date_initial - pd.Timedelta(days=180)  # âœ… 6 ay geriye
            
            # Find new cutoff position
            mask = df_sorted["_install_clean"] <= cutoff_date_adjusted
            if mask.any():
                cutoff_pos = mask.sum()
                
                # Verify improvement
                test_events_adjusted = df_sorted.iloc[cutoff_pos:]["event"].mean()

                if test_events_adjusted > test_events_initial:
                    if logger:
                        logger.info(f"[SPLIT] Adjusted cutoff: test events {test_events_initial:.1%} â†’ {test_events_adjusted:.1%}")
                else:
                    cutoff_pos = cutoff_pos_initial  # Rollback if no improvement
                    if logger:
                        logger.warning(f"[SPLIT] Adjustment failed, keeping original cutoff")
            else:
                cutoff_pos = cutoff_pos_initial
        else:
            cutoff_pos = cutoff_pos_initial
    else:
        cutoff_pos = cutoff_pos_initial
    
    # Split (strict: no overlap)
    # Ensure cutoff date is EXCLUSIVE for train (train < cutoff_date)
    cutoff_date_value = df_sorted.iloc[cutoff_pos]["_install_clean"]

    # Train: strictly before cutoff
    train_mask = df_sorted["_install_clean"] < cutoff_date_value
    # Test: cutoff and after
    test_mask = df_sorted["_install_clean"] >= cutoff_date_value

    train_labels = df_sorted[train_mask].index.values
    test_labels = df_sorted[test_mask].index.values

    if logger:
        cutoff_date = df_sorted.iloc[cutoff_pos]["_install_clean"]
        logger.info(f"[TEMPORAL SPLIT] Cutoff: {cutoff_date.date()}")
        logger.info(f"[TEMPORAL SPLIT] Train: {len(train_labels)} | Test: {len(test_labels)}")

        # âœ… DEBUG: Tarihleri detaylÄ± yazdÄ±r
        train_dates = df_sorted.iloc[:cutoff_pos]["_install_clean"]
        test_dates = df_sorted.iloc[cutoff_pos:]["_install_clean"]
        logger.info(f"[DEBUG] Train date range: {train_dates.min().date()} -> {train_dates.max().date()}")
        logger.info(f"[DEBUG] Test date range: {test_dates.min().date()} -> {test_dates.max().date()}")

        if "event" in df.columns:
            train_ev = df.loc[train_labels, "event"].mean()
            test_ev = df.loc[test_labels, "event"].mean()
            logger.info(f"[TEMPORAL SPLIT] Train events: {train_ev:.1%} | Test events: {test_ev:.1%}")

            # âœ… UYARI: Event rate mismatch kontrolÃ¼ (GÃœNCELLENDÄ°)
            if train_ev > 0 and test_ev > 0:
                event_rate_ratio = test_ev / train_ev

                # âœ… REVERSED CHECK: Test >> Train ise kritik UYARI (fallback YOK)
                if event_rate_ratio > 2.0:
                    logger.error(
                        f"[CRITICAL] TEST EVENT RATE ANOMALY DETECTED!\n"
                        f"  Train: {train_ev:.1%} | Test: {test_ev:.1%} | Ratio: {event_rate_ratio:.2f}x\n"
                        f"  -> Test has {event_rate_ratio:.1f}x MORE failures than train!\n"
                        f"  LIKELY CAUSES:\n"
                        f"    1. Temporal split REVERSED (old equipment in test)\n"
                        f"    2. Data quality issue (mass failure event in recent period)\n"
                        f"    3. Kurulum_Tarihi column has errors\n"
                        f"  PROCEEDING WITH TEMPORAL SPLIT (fallback disabled)\n"
                        f"  Model metrics may be unreliable - review data quality!"
                    )
                elif event_rate_ratio < 0.7 or event_rate_ratio > 1.3:
                    logger.warning(
                        f"[SPLIT WARNING] Event rate mismatch detected!\n"
                        f"  Train: {train_ev:.1%} | Test: {test_ev:.1%} | Ratio: {event_rate_ratio:.2f}\n"
                        f"  Possible causes:\n"
                        f"    - Right censoring: Newer assets haven't failed yet\n"
                        f"    - Data quality: Different failure patterns over time\n"
                        f"  Recommendation: Consider using stratified split or adjusting cutoff date"
                    )

    # âœ… POST-SPLIT BALANCING (IEEE PHM Standard 1:5)
    # Critical fix: Balance AFTER split to maintain consistent event rates
    if apply_balancing and "event" in df.columns:
        from sklearn.utils import resample

        target_ratio = 5  # 1:5 faulty:healthy
        random_state = 42

        if logger:
            logger.info(f"[BALANCING] Applying post-split balancing (1:{target_ratio})")

        # Balance train set
        train_df = df.loc[train_labels].copy()
        train_faulty = train_df[train_df["event"] == 1]
        train_healthy = train_df[train_df["event"] == 0]

        n_train_faulty = len(train_faulty)
        n_train_healthy_target = n_train_faulty * target_ratio

        if len(train_healthy) > n_train_healthy_target:
            train_healthy_sampled = resample(train_healthy,
                                             n_samples=n_train_healthy_target,
                                             random_state=random_state,
                                             replace=False)
            train_balanced = pd.concat([train_faulty, train_healthy_sampled])
            train_labels = train_balanced.index.values

            if logger:
                logger.info(f"[BALANCING] Train: {len(train_faulty)} faulty | {len(train_healthy)} -> {n_train_healthy_target} healthy")

        # Balance test set
        test_df = df.loc[test_labels].copy()
        test_faulty = test_df[test_df["event"] == 1]
        test_healthy = test_df[test_df["event"] == 0]

        n_test_faulty = len(test_faulty)
        n_test_healthy_target = n_test_faulty * target_ratio

        if len(test_healthy) > n_test_healthy_target:
            test_healthy_sampled = resample(test_healthy,
                                           n_samples=n_test_healthy_target,
                                           random_state=random_state,
                                           replace=False)
            test_balanced = pd.concat([test_faulty, test_healthy_sampled])
            test_labels = test_balanced.index.values

            if logger:
                logger.info(f"[BALANCING] Test: {len(test_faulty)} faulty | {len(test_healthy)} -> {n_test_healthy_target} healthy")

        # Verify balanced event rates
        if logger:
            train_ev_balanced = df.loc[train_labels, "event"].mean()
            test_ev_balanced = df.loc[test_labels, "event"].mean()
            ratio_balanced = test_ev_balanced / train_ev_balanced if train_ev_balanced > 0 else 0

            logger.info(f"[BALANCING] Post-balance event rates:")
            logger.info(f"  Train: {train_ev_balanced:.1%} | Test: {test_ev_balanced:.1%} | Ratio: {ratio_balanced:.2f}x")

            if 0.5 <= ratio_balanced <= 2.0:
                logger.info(f"[BALANCING] Event rates are now balanced!")
            else:
                logger.warning(f"[BALANCING] Event rates still imbalanced after balancing")

    return train_labels, test_labels

# =============================================================================
# DATA LOADING
# =============================================================================
# Ä°zmir ve Manisa BÃ¶lgesi Ä°lÃ§e KodlarÄ±
ILCE_ID_MAPPING = {
    # --- MANÄ°SA ---
    1118: 'Ahmetli', 1119: 'Akhisar', 1127: 'Alasehir', 1269: 'Demirci',
    1362: 'Gordes', 1470: 'Kirkagac', 1489: 'Kula', 1590: 'Salihli',
    1600: 'Sarigol', 1606: 'Saruhanli', 1613: 'Selendi', 1634: 'Soma',
    1682: 'Turgutlu', 1751: 'Sehzadeler', 1752: 'Yunusemre', 1965: 'Koprubasi',
    # --- Ä°ZMÄ°R ---
    1109: 'Aliaga', 1165: 'Bayindir', 1188: 'Bergama', 1205: 'Bornova',
    1216: 'Buca', 1251: 'Cesme', 1280: 'Dikili', 1334: 'Foca',
    1432: 'Karaburun', 1448: 'Karsiyaka', 1461: 'Kemalpasa', 1467: 'Kinik',
    1477: 'Kiraz', 1500: 'Menemen', 1542: 'Odemis', 1611: 'Seferihisar',
    1612: 'Selcuk', 1677: 'Tire', 1689: 'Torbali', 1703: 'Urla',
    1780: 'Beydag', 1801: 'Konak', 1826: 'Menderes', 1888: 'Balcova',
    1889: 'Cigli', 1890: 'Gaziemir', 1891: 'Narlidere', 1892: 'Guzelbahce',
    2006: 'Bayrakli', 2007: 'Karabaglar'
}
def load_fault_data(logger: logging.Logger) -> pd.DataFrame:
    """Load and clean fault records"""
    path = DATA_PATHS["fault_data"]
    logger.info(f"[LOAD] Fault data: {path}")

    df = pd.read_excel(path)
    df.columns = [c.strip() for c in df.columns]

    # --- GÃœNCELLEME: Lokasyon SÃ¼tunlarÄ±nÄ± Ekle ---
    base_cols = ["cbs_id", "Åebeke Unsuru", "Sebekeye_Baglanma_Tarihi",
                 "started at", "ended at", "duration time", "cause code"]
    
    # Mevcut bakÄ±m sÃ¼tunlarÄ± + Sizin belirttiÄŸiniz YENÄ° lokasyon sÃ¼tunlarÄ±
    extra_cols = ["BakÄ±m SayÄ±sÄ±", "Son BakÄ±m Ä°ÅŸ Emri Tarihi", "MARKA",
                  #"kVA_Rating",
                  "component_voltage", "voltage_level",
                  "X_KOORDINAT", "Y_KOORDINAT", "Ä°lÃ§e"]  # <--- EKLENDÄ°

    use_cols = [c for c in base_cols + extra_cols if c in df.columns]
    df = df[use_cols].copy()

    # Rename Mapping
    df = df.rename(columns={
        "Åebeke Unsuru": "Ekipman_Tipi",
        "Sebekeye_Baglanma_Tarihi": "Kurulum_Tarihi",
        "duration time": "SÃ¼re_Ham",
        "BakÄ±m SayÄ±sÄ±": "Bakim_Sayisi",
        "MARKA": "Marka",
        "component_voltage": "Gerilim_Seviyesi",
        "voltage_level": "Gerilim_Sinifi",
        # --- LOKASYON MAPPING ---
        "X_KOORDINAT": "Longitude",  # Genelde X BoylamdÄ±r
        "Y_KOORDINAT": "Latitude",   # Genelde Y Enlemdir
        "Ä°lÃ§e": "Ilce"
    })

    # Parse dates
    df["Kurulum_Tarihi"] = df["Kurulum_Tarihi"].apply(parse_date_safely)
    df["started at"] = df["started at"].apply(parse_date_safely)
    df["ended at"] = df["ended at"].apply(parse_date_safely)
    df["SÃ¼re_Dakika"] = convert_duration_minutes(df["SÃ¼re_Ham"], logger)
    df["Ekipman_Tipi"] = clean_equipment_type(df["Ekipman_Tipi"])

    # Filter future dates (data quality check)
    today = pd.Timestamp.now()
    future_faults = (df["started at"] > today).sum()
    if future_faults > 0:
        logger.warning(f"[DATA QUALITY] {future_faults} gelecek tarihli arÄ±za kaydÄ± bulundu ve filtrelendi.")
        df = df[df["started at"] <= today].copy()

    # Filter invalid records
    original = len(df)
    df = df[df["cbs_id"].notna()].copy()
    df["cbs_id"] = df["cbs_id"].astype(str).str.lower().str.strip()

    df = df[
        df["Kurulum_Tarihi"].notna() &
        df["started at"].notna() &
        df["ended at"].notna() &
        df["SÃ¼re_Dakika"].notna()
    ].copy()
    # KoordinatlarÄ± sayÄ±ya Ã§evirmeyi garantiye al (HatalÄ± text varsa NaN olsun)
    for col in ["Longitude", "Latitude"]:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors="coerce")
    logger.info(f"[LOAD] Fault records: {len(df)}/{original} ({100*len(df)/original:.1f}%)")

    # Save intermediate output
    df.to_csv(INTERMEDIATE_PATHS["fault_events_clean"], index=False, encoding="utf-8-sig")
    logger.info(f"[SAVE] Intermediate: {INTERMEDIATE_PATHS['fault_events_clean']}")
    return df

def load_healthy_data(logger: logging.Logger) -> pd.DataFrame:
    """Load healthy equipment (no fault history)"""
    path = DATA_PATHS["healthy_data"]
    logger.info(f"[LOAD] Healthy data: {path}")

    df = pd.read_excel(path)
    df.columns = [c.strip() for c in df.columns]

    if "cbs_id" not in df.columns:
        df = df.rename(columns={"ID": "cbs_id"})

    df["cbs_id"] = df["cbs_id"].astype(str).str.lower().str.strip()
    
    # --- GÃœNCELLEME BURADA BAÅLIYOR ---
    # ArÄ±za verisiyle aynÄ± isimlere eÅŸitliyoruz
    # --- GÃœNCELLEME: Rename HaritasÄ± ---
    rename_map = {
        "Åebeke Unsuru": "Ekipman_Tipi",
        "Sebekeye_Baglanma_Tarihi": "Kurulum_Tarihi",
        "MARKA": "Marka",
        "BakÄ±m SayÄ±sÄ±": "Bakim_Sayisi",
        "component_voltage": "Gerilim_Seviyesi",
        "voltage_level": "Gerilim_Sinifi",
        #"kVA_Rating": "kVA_Rating",
        # --- YENÄ° LOKASYONLAR ---
        "X_KOORDINAT": "Longitude",
        "Y_KOORDINAT": "Latitude",
        "ADR_ILCE_ID": "Ilce_ID",   # Ã–nce ID olarak alÄ±yoruz
    }
    df = df.rename(columns=rename_map)
    # --- YENÄ°: ID'den Ä°sme Ã‡evirme ---
    if "Ilce_ID" in df.columns:
        # map fonksiyonu ile ID'leri isme Ã§evir, bulamazsa 'Bilinmiyor' yazar
        df["Ilce"] = df["Ilce_ID"].map(ILCE_ID_MAPPING).fillna("Bilinmiyor")
        
        # ArtÄ±k ID kolonuna ihtiyacÄ±mÄ±z kalmadÄ±ysa atabiliriz veya tutabiliriz
        # df = df.drop(columns=["Ilce_ID"])
    else:
        df["Ilce"] = "Unknown"
    df["Kurulum_Tarihi"] = df["Kurulum_Tarihi"].apply(parse_date_safely)
    df["Ekipman_Tipi"] = clean_equipment_type(df["Ekipman_Tipi"])
    # Koordinat temizliÄŸi
    for col in ["Longitude", "Latitude"]:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors="coerce")
    # Gereksiz/BoÅŸ kayÄ±tlarÄ± temizle
    df = df[df["Kurulum_Tarihi"].notna() & df["cbs_id"].notna()].copy()

    logger.info(f"[LOAD] Healthy equipment: {len(df)}")
    
    # Kaydederken de bu yeni sÃ¼tunlarÄ±n olduÄŸundan emin oluyoruz
    df.to_csv(INTERMEDIATE_PATHS["healthy_equipment_clean"], index=False, encoding="utf-8-sig")
    logger.info(f"[SAVE] Intermediate: {INTERMEDIATE_PATHS['healthy_equipment_clean']}")

    return df

# =============================================================================
# STEP 01: EQUIPMENT MASTER + SURVIVAL BASE
# =============================================================================

# =============================================================================
# ğŸ”§ BAKIM VERÄ°SÄ° STRATEJÄ°SÄ° (MAINTENANCE STRATEGY)
# =============================================================================
# Sorun:
#   Veri setimizde 'Bakim_Sayisi' sÃ¼tunu sÄ±kÃ§a boÅŸ (NaN) geliyor.
#   NaN deÄŸerlerini 0 (SÄ±fÄ±r) ile doldurmak HATALIDIR. Ã‡Ã¼nkÃ¼:
#   - 0: "Kesinlikle bakÄ±m yapÄ±lmadÄ±" (KÃ¶tÃ¼ bir durum olabilir)
#   - NaN: "BakÄ±m yapÄ±lÄ±p yapÄ±lmadÄ±ÄŸÄ±nÄ± bilmiyoruz" (Belirsiz bir durum)
#
# Ã‡Ã¶zÃ¼m:
#   Modelin bu iki durumu ayÄ±rt edebilmesi iÃ§in 'Bakim_Sayisi' sÃ¼tununu
#   iki yeni Ã¶zelliÄŸe (feature) dÃ¶nÃ¼ÅŸtÃ¼rÃ¼yoruz:
#
#   1. Bakim_Verisi_Var (Flag): 
#      - 1: BakÄ±m verisi sistemde kayÄ±tlÄ±.
#      - 0: BakÄ±m verisi yok / bilinmiyor.
#
#   2. Bakim_Sayisi_Safe (Value):
#      - Pozitif SayÄ±lar (1, 2, 5...): GerÃ§ek bakÄ±m sayÄ±sÄ±.
#      - 0: HiÃ§ bakÄ±m yapÄ±lmamÄ±ÅŸ (Veri var ama sayÄ± 0).
#      - -1: Bilinmiyor (NaN).
#
#   Neden?
#   AÄŸaÃ§ tabanlÄ± modeller (XGBoost, Random Forest), -1 ile 0 arasÄ±ndaki farkÄ±
#   Ã¶ÄŸrenebilir. BÃ¶ylece "BakÄ±msÄ±zlÄ±k Riski" ile "Veri EksikliÄŸi Riski" birbirinden ayrÄ±lÄ±r.
# =============================================================================
def build_equipment_master(
    df_fault: pd.DataFrame,
    df_healthy: pd.DataFrame,
    logger: logging.Logger,
    data_end_date: pd.Timestamp
) -> pd.DataFrame:
    """Combine fault + healthy equipment into master registry"""
    
    # 1. Ortak Aggregation KurallarÄ±
    agg_cols = {
        "Kurulum_Tarihi": ("Kurulum_Tarihi", "min"),
        "Ekipman_Tipi": ("Ekipman_Tipi", "first"),
        "Marka": ("Marka", "first"),
        "Gerilim_Seviyesi": ("Gerilim_Seviyesi", "max"),
        "Gerilim_Sinifi": ("Gerilim_Sinifi", "first"),
        "Bakim_Sayisi": ("Bakim_Sayisi", "max"),
        "Longitude": ("Longitude", "mean"), 
        "Latitude": ("Latitude", "mean"),
        "Ilce": ("Ilce", "first")
    }

    # 2. ArÄ±zalÄ± EkipmanlarÄ± Ã–zetle
    fault_agg_rules = agg_cols.copy()
    fault_agg_rules.update({
        "Fault_Count": ("cbs_id", "size"),
        "Ilk_Ariza_Tarihi": ("started at", "min"),
        "Son_Ariza_Tarihi": ("started at", "max")
    })
    
    # --- DÃœZELTME BAÅLANGICI ---
    # HATA Ã‡Ã–ZÃœMÃœ: Sadece df_fault iÃ§inde GERÃ‡EKTEN VAR OLAN sÃ¼tunlarÄ± kurallara dahil et.
    # EÄŸer 'Latitude' yÃ¼klenemediyse, burada iÅŸlemeye Ã§alÄ±ÅŸÄ±p hata vermesin.
    final_fault_rules = {}
    for col, rule in fault_agg_rules.items():
        # KuralÄ±n anahtarÄ± (Ã¶rn: 'Latitude') dataframe sÃ¼tunlarÄ±nda var mÄ±?
        # Veya bu bir tÃ¼retilen sÃ¼tun mu (Ã¶rn: 'Fault_Count')?
        if col in df_fault.columns or col == "Fault_Count":
            final_fault_rules[col] = rule
            
    fault_agg = df_fault.groupby("cbs_id").agg(**final_fault_rules).reset_index()
    # --- DÃœZELTME BÄ°TÄ°ÅÄ° ---
    
    # 3. SaÄŸlam EkipmanlarÄ± Ã–zetle
    # AynÄ± gÃ¼venli mantÄ±ÄŸÄ± burada da uyguluyoruz
    healthy_agg_rules = {}
    for col, rule in agg_cols.items():
        if col in df_healthy.columns: 
            healthy_agg_rules[col] = rule
            
    healthy_agg = df_healthy.groupby("cbs_id").agg(**healthy_agg_rules).reset_index()
    healthy_agg["Fault_Count"] = 0

    # ğŸ¯ DATA BALANCING (Config-Driven)
    # IEEE PHM Best Practice: 1:3 to 1:5 for predictive maintenance
    n_faulty = len(fault_agg)

    if BALANCE_ENABLED and BALANCE_METHOD == "undersample":
        n_healthy_target = n_faulty * BALANCE_TARGET_RATIO

        if len(healthy_agg) > n_healthy_target:
            logger.info(f"[BALANCING] Target ratio: 1:{BALANCE_TARGET_RATIO} (IEEE PHM Standard)")
            logger.info(f"[BALANCING] Undersampling healthy assets: {len(healthy_agg)} â†’ {n_healthy_target}")
            healthy_agg = healthy_agg.sample(n=n_healthy_target, random_state=BALANCE_RANDOM_STATE).reset_index(drop=True)
        else:
            logger.info(f"[BALANCING] Healthy assets already below target: {len(healthy_agg)} < {n_healthy_target}")
    else:
        logger.info(f"[BALANCING] Disabled - using all {len(healthy_agg)} healthy assets")

    # 4. BirleÅŸtir
    all_eq = pd.concat([fault_agg, healthy_agg], ignore_index=True)
    
    # Ã‡akÄ±ÅŸmalarÄ± Temizle (Bir ekipman hem arÄ±zalÄ± hem saÄŸlam listesinde olamaz ama varsa arÄ±zalÄ±yÄ± koru)
    all_eq = all_eq.sort_values(["cbs_id", "Fault_Count"], ascending=[True, False]) \
                   .drop_duplicates("cbs_id", keep="first")
    
    # Nadir Tipleri Temizle
    counts = all_eq["Ekipman_Tipi"].value_counts()
    rare = counts[counts < MIN_EQUIPMENT_PER_CLASS].index.tolist()
    if rare:
        logger.info(f"[COLLAPSE] Rare types â†’ 'Diger': {rare}")
        all_eq.loc[all_eq["Ekipman_Tipi"].isin(rare), "Ekipman_Tipi"] = "Diger"
    
    # --- YENÄ° KOD (YAPIÅTIRIN) ---
    if "Bakim_Sayisi" in all_eq.columns:
        # 1. Flag: Bu varlÄ±ÄŸÄ±n bakÄ±m bilgisini biliyor muyuz? (1: Evet, 0: HayÄ±r/NaN)
        all_eq["Bakim_Verisi_Var"] = all_eq["Bakim_Sayisi"].notna().astype(int)
        
        # 2. SayÄ±: NaN olanlarÄ± -1 yapÄ±yoruz.
        # Neden -1? Ã‡Ã¼nkÃ¼ AÄŸaÃ§ tabanlÄ± modeller (RSF, XGBoost) -1'i "Bilinmiyor", 0'Ä± "HiÃ§ BakÄ±m Yok" olarak ayÄ±rabilir.
        all_eq["Bakim_Sayisi_Safe"] = all_eq["Bakim_Sayisi"].fillna(-1)
        
        logger.info(f"[MASTER] BakÄ±m verisi iÅŸlendi. Bilinen kayÄ±t: {all_eq['Bakim_Verisi_Var'].sum()}")
    else:
        # SÃ¼tun hiÃ§ yoksa varsayÄ±lanlarÄ± oluÅŸtur
        all_eq["Bakim_Verisi_Var"] = 0
        all_eq["Bakim_Sayisi_Safe"] = -1

    logger.info(f"[MASTER] Equipment registry: {len(all_eq)} assets")
    return all_eq

# =============================================================================
# UPDATED build_survival_base 
# =============================================================================
# =============================================================================
# â³ SURVIVAL BASE DATASET (YAÅAM SÃœRESÄ° TABLOSU & SOL KESÄ°LME)
# =============================================================================
# Bu fonksiyon, Survival Analizi'nin bel kemiÄŸi olan (Duration, Event) Ã§iftini oluÅŸturur.
# Ä°statistiksel doÄŸruluÄŸu saÄŸlamak iÃ§in 3 kritik iÅŸlem yapar:
#
# ğŸ¯ 1. GerÃ§ek ArÄ±za TanÄ±mÄ± (Event = 1):
#    - Sigorta atmasÄ± (Fuse Trip) gibi koruma operasyonlarÄ± "ArÄ±za" sayÄ±lmaz.
#    - Sadece fiziksel hasarlar (Tel kopmasÄ±, Trafo yanmasÄ±) "Ã–lÃ¼m" (Event=1) kabul edilir.
#
# ğŸ“ 2. Sol Kesilme (Left Truncation / Delayed Entry):
#    - SORUN: Veri setimiz 2021'de baÅŸlÄ±yor ama ÅŸebekede 1990 model trafo var.
#    - RÄ°SK: Modele "Bu trafo 1990-2021 arasÄ± hiÃ§ bozulmadÄ±" dersek (Survivorship Bias),
#      model eski varlÄ±klarÄ± "Ã¶lÃ¼msÃ¼z" sanar.
#    - Ã‡Ã–ZÃœM: 'entry_days' hesaplÄ±yoruz. Modele diyoruz ki:
#      "Bu varlÄ±k 1990'da doÄŸdu ama biz onu 2021'de (yani 11.000 gÃ¼nlÃ¼kken) izlemeye baÅŸladÄ±k."
#      Model, 0-11.000 gÃ¼n arasÄ±ndaki saÄŸ kalÄ±mÄ± baÅŸarÄ± hanesine yazmaz, sadece sonrasÄ±nÄ± deÄŸerlendirir.
#
# â±ï¸ 3. Ã–mÃ¼r (Duration):
#    - Ã–lenler iÃ§in: Kurulum Tarihi -> Ä°lk GerÃ§ek ArÄ±za Tarihi
#    - YaÅŸayanlar iÃ§in: Kurulum Tarihi -> Analiz Tarihi (Verinin BittiÄŸi GÃ¼n)
# =============================================================================
def build_survival_base(
    equipment_master: pd.DataFrame,
    df_fault: pd.DataFrame,
    logger,
    data_end_date
) -> pd.DataFrame:
    """
    Create survival dataset - uses ALL fault records

    âœ… UPDATED: Domain expert verified all cause codes are valid failures
    """

    # Data quality already checked in load_fault_data()
    df_fault_real = df_fault.copy()
    
    # First REAL failure per equipment
    first_real_fail = df_fault_real.groupby("cbs_id")["started at"].min().rename("Ilk_Gercek_Ariza_Tarihi")
    
    # Keep essential columns
    keep_cols = ["cbs_id", "Ekipman_Tipi", "Kurulum_Tarihi", "Fault_Count"]
    if "Gerilim_Sinifi" in equipment_master.columns:
        keep_cols.append("Gerilim_Sinifi")
    if "Marka" in equipment_master.columns:
        keep_cols.append("Marka")
    
    df = equipment_master[[c for c in keep_cols if c in equipment_master.columns]].copy()
    df = df.merge(first_real_fail, on="cbs_id", how="left")
    
    # Event = 1 if equipment had REAL failure (not just protective operation)
    df["event"] = df["Ilk_Gercek_Ariza_Tarihi"].notna().astype(int)
    
    # Duration to REAL failure (or censoring)
    # --- NEW: Calculate Entry Time (Left Truncation) ---
    # If installed BEFORE 2021, it enters risk set in 2021 (age > 0)
    # If installed AFTER 2021, it enters risk set at installation (age = 0)
    df["entry_days"] = np.where(
        df["Kurulum_Tarihi"] < OBSERVATION_START_DATE,
        (OBSERVATION_START_DATE - df["Kurulum_Tarihi"]).dt.days,
        0
    )
    
    # Duration is still calculated from Installation Date
    # But now Cox knows we didn't watch the first 'entry_days'
    df["duration_days"] = np.where(
        df["event"] == 1,
        (df["Ilk_Gercek_Ariza_Tarihi"] - df["Kurulum_Tarihi"]).dt.days,
        (data_end_date - df["Kurulum_Tarihi"]).dt.days
    )
    
    # Safety: duration must be > entry
    df = df[df["duration_days"] > df["entry_days"]].copy()
    
    return df

# =============================================================================
# STEP 02: FEATURE ENGINEERING - SINGLE DATAFRAME APPROACH
# =============================================================================
# =============================================================================
# ğŸ“‰ CHRONIC FEATURE ENGINEERING (KRONÄ°K ARIZA VE MTBF ANALÄ°ZÄ°)
# =============================================================================
# Bu fonksiyon, varlÄ±klarÄ±n "kÄ±sa vadeli" saÄŸlÄ±k durumunu 4 farklÄ± aÃ§Ä±dan analiz eder.
#
# ğŸ§  1. Exponential Decay (Ãœstel Bozunma): 
#    - "DÃ¼n yaÅŸanan arÄ±za, 3 ay Ã¶nceki arÄ±zadan daha tehlikelidir."
#    - YakÄ±n geÃ§miÅŸteki arÄ±zalara daha yÃ¼ksek aÄŸÄ±rlÄ±k vererek (Weight=1.0 vs 0.2)
#      kriz geÃ§irmekte olan varlÄ±klarÄ± Ã¶ne Ã§Ä±karÄ±r.
#
# ğŸ§® 2. Bayesian MTBF (Mean Time Between Failures):
#    - SORUN: Klasik MTBF = (SÃ¼re / ArÄ±za SayÄ±sÄ±). HiÃ§ arÄ±za yapmamÄ±ÅŸ varlÄ±kta
#      bÃ¶len 0 olduÄŸu iÃ§in sonuÃ§ sonsuz veya tanÄ±msÄ±z Ã§Ä±kar.
#    - Ã‡Ã–ZÃœM: FormÃ¼le "Sanal BaÅŸlangÄ±Ã§ DeÄŸerleri" (Priors) eklenir.
#      FormÃ¼l: (Pencere SÃ¼resi + 30 gÃ¼n) / (ArÄ±za SayÄ±sÄ± + 1).
#    - SONUÃ‡: HiÃ§ arÄ±zasÄ± olmayan varlÄ±klarÄ±n bile mantÄ±klÄ± bir risk skoru olur
#      ve model onlarÄ± kÄ±yaslayabilir.
#
# ğŸ“… 3. Annualized Rate (YÄ±llÄ±klandÄ±rÄ±lmÄ±ÅŸ HÄ±z):
#    - 90 gÃ¼nlÃ¼k performansÄ± 1 yÄ±la projete eder (Ã–rn: 3 ayda 2 arÄ±za = YÄ±lda 8 arÄ±za).
#
# ğŸš© 4. Chronic Flag:
#    - Belirli bir eÅŸiÄŸi (Ã¶rn: 3 arÄ±za) aÅŸan varlÄ±klarÄ± "Kronik Sorunlu" (1) olarak etiketler.
#    - IEEE 1366 prensiplerine benzer ÅŸekilde, bu Ã¶zellik modelin en gÃ¼Ã§lÃ¼ sinyallerinden biridir.
# =============================================================================



# =============================================================================
def compute_chronic_features(
    df_fault: pd.DataFrame,
    t_ref: pd.Timestamp,
    logger: logging.Logger
) -> pd.DataFrame:
    """Chronic equipment detection (Bayesian MTBF + Decay)"""
    
    window_start = t_ref - pd.Timedelta(days=CHRONIC_WINDOW_DAYS)
    fe = df_fault[df_fault["started at"] >= window_start].copy()
    
    # EÄŸer hiÃ§ arÄ±za yoksa boÅŸ dÃ¶n
    if len(fe) == 0:
        logger.warning(f"[CHRONIC] No faults in window")
        # SÃ¼tun isimlerini eksiksiz tanÄ±mlayÄ±n
        cols = ["cbs_id", "Ariza_Sayisi_90g", "Chronic_Rate_Yillik", 
                "Chronic_Decay_Skoru", "Chronic_Flag", "MTBF_Bayes_Gun"]
        return pd.DataFrame(columns=cols)
    
    # 1. ArÄ±za SayÄ±larÄ±
    counts = fe.groupby("cbs_id").size().rename("Ariza_Sayisi_90g")
    
    # 2. Decay Skoru
    age_days = (t_ref - fe["started at"]).dt.days.clip(lower=0)
    fe["decay"] = np.exp(-0.05 * age_days)
    decay_score = fe.groupby("cbs_id")["decay"].sum().rename("Chronic_Decay_Skoru")
    
    # 3. YÄ±llÄ±k Oran
    rate = (counts / (CHRONIC_WINDOW_DAYS / 365.25)).rename("Chronic_Rate_Yillik")
    
    # --- EKLENEN KISIM: BAYESIAN MTBF ---
    # FormÃ¼l: (Pencere SÃ¼resi + Beta) / (ArÄ±za SayÄ±sÄ± + Alfa)
    # Alfa=1 (Sanal 1 arÄ±za), Beta=30 (Sanal 1 ay Ã¶mÃ¼r) varsayalÄ±m.
    # Bu, 0 arÄ±zasÄ± olanÄ± sonsuz yapmaz, "HenÃ¼z bozulmadÄ± ama riskli olabilir" seviyesinde tutar.
    alpha = 1
    beta = 30
    mtbf = ((CHRONIC_WINDOW_DAYS + beta) / (counts + alpha)).rename("MTBF_Bayes_Gun")
    # ------------------------------------

    # 4. Kronik BayraÄŸÄ±
    chronic_flag = ((counts >= CHRONIC_THRESHOLD_EVENTS) | (rate >= CHRONIC_MIN_RATE)).astype(int).rename("Chronic_Flag")
    
    # Ã‡Ä±ktÄ±larÄ± BirleÅŸtir (mtbf eklendi)
    out = pd.concat([counts, rate, decay_score, chronic_flag, mtbf], axis=1).reset_index()
    
    logger.info(f"[CHRONIC] Window: {CHRONIC_WINDOW_DAYS}d | Chronic assets: {chronic_flag.sum()}")
    return out

# =============================================================================
# ğŸ•’ TEMPORAL FEATURES & OBSERVABILITY (ZAMANSAL Ã–ZELLÄ°KLER & GÃ–ZLENEBÄ°LÄ°RLÄ°K)
# =============================================================================
# Bu fonksiyon, statik varlÄ±k verisine "Zaman Boyutunu" ve "GeÃ§miÅŸ Ä°statistiklerini" ekler.
#
# ğŸ” 1. GÃ¶zlenebilirlik (Observability) - Bias Ã–nleme:
#    - SORUN: 30 yaÅŸÄ±ndaki bir trafoyu sadece son 3 yÄ±ldÄ±r (2021'den beri) izliyor olabiliriz.
#      Model bunu bilmezse, varlÄ±ÄŸÄ±n 30 yÄ±ldÄ±r sorunsuz Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± sanar.
#    - Ã‡Ã–ZÃœM: 'Observation_Ratio' (Ä°zlenen SÃ¼re / Toplam YaÅŸ) hesaplanÄ±r.
#    - SONUÃ‡: Model, "Legacy" (Eski ama verisi az) varlÄ±klar ile "Yeni" (TÃ¼m hayatÄ± bilinen)
#      varlÄ±klarÄ± ayÄ±rt etmeyi Ã¶ÄŸrenir.
#
# ğŸ“Š 2. Kronik Veri Entegrasyonu (Merge):
#    - compute_chronic_features fonksiyonundan gelen 4 kritik metriÄŸi ana tabloya iÅŸler:
#      a. Chronic_Flag: Kronik sorunlu mu? (1/0)
#      b. Chronic_Decay_Skoru: ArÄ±zalar ne kadar taze? (YakÄ±n zamana aÄŸÄ±rlÄ±k verir)
#      c. Chronic_Rate_Yillik: YÄ±llÄ±k arÄ±za hÄ±zÄ±.
#      d. MTBF_Bayes_Gun: (YENÄ° âœ…) SÄ±fÄ±r arÄ±zalÄ± varlÄ±klar iÃ§in bile hesaplanan,
#         istatistiksel olarak dÃ¼zeltilmiÅŸ "ArÄ±zalar ArasÄ± Ortalama SÃ¼re".
# =============================================================================
def add_survival_columns_inplace(
    df: pd.DataFrame,
    df_fault_filtered: pd.DataFrame,
    data_end_date: pd.Timestamp,
    observation_start_date: pd.Timestamp,  # <--- NEW ARGUMENT
    logger: logging.Logger
) -> pd.DataFrame:
    """
    Add event/duration AND entry_days (Left Truncation) to equipment_master.
    """
    # 1. First REAL failure per equipment
    first_fail = df_fault_filtered.groupby("cbs_id")["started at"].min()
    
    # Add to existing dataframe
    df["Ilk_Gercek_Ariza_Tarihi"] = df["cbs_id"].map(first_fail)
    
    # 2. Calculate event flag
    df["event"] = df["Ilk_Gercek_Ariza_Tarihi"].notna().astype(int)
    
    # 3. Calculate duration (End - Start)
    # If Failed: Duration = Failure Date - Install Date
    # If Healthy: Duration = Analysis End Date - Install Date
    
    failure_duration = (df["Ilk_Gercek_Ariza_Tarihi"] - df["Kurulum_Tarihi"]).dt.days
    healthy_duration = (data_end_date - df["Kurulum_Tarihi"]).dt.days
    
    df["duration_days"] = np.where(
        df["event"] == 1,
        failure_duration,
        healthy_duration
    )
    # Bu mantÄ±ÄŸÄ± feature engineering fonksiyonuna ekleyin veya gÃ¼ncelleyin
    df = df.dropna(subset=['Kurulum_Tarihi'])  # YaÅŸÄ± olmayan arÄ±zayÄ± modelleyemeyiz
    # --- NEW: DELAYED ENTRY (LEFT TRUNCATION) ---
    # Assets installed BEFORE we started recording (e.g. 2021) enter the risk set LATE.
    # We didn't observe them from Install Date to 2021, so we calculate that gap.
    
    df["entry_days"] = np.where(
        df["Kurulum_Tarihi"] < observation_start_date,
        (observation_start_date - df["Kurulum_Tarihi"]).dt.days,
        0
    )
    
    # CLEANUP:
    # 1. Fill NaNs (if any duration calc failed)
    df["duration_days"] = df["duration_days"].fillna(0)
    df["entry_days"] = df["entry_days"].fillna(0)
    
    # 2. Handle Logic Errors & Clamp
    # We cap max duration to 60 years to avoid outliers affecting scalers
    df["duration_days"] = df["duration_days"].clip(upper=60*365)
    
    # Logic Check: 
    # Duration must be positive.
    # Duration must be > Entry Time (Asset cannot fail BEFORE we started watching it)
    valid_mask = (df["duration_days"] > 0) & (df["duration_days"] > df["entry_days"])
    
    dropped_count = (~valid_mask).sum()
    if dropped_count > 0:
        logger.warning(f"[SURVIVAL] Dropping {dropped_count} assets with invalid duration (Failed before observation start or data error)")
        df = df[valid_mask].copy()
    
    logger.info(f"[SURVIVAL] Added to master: {len(df)} assets, {df['event'].sum()} events ({100*df['event'].mean():.1f}%)")
    return df

def add_temporal_features_inplace(
    df: pd.DataFrame,
    t_ref: pd.Timestamp,
    chronic_df: pd.DataFrame,
    observation_start_date: pd.Timestamp,
    logger: logging.Logger
) -> pd.DataFrame:
    """
    Add temporal features directly to existing dataframe IN-PLACE.
    Includes fix for NumPy Timestamp comparison error.
    """
    # 1. Calculate Age (Tref_Yas_Gun)
    # Ensure datetime format just in case
    if not pd.api.types.is_datetime64_any_dtype(df["Kurulum_Tarihi"]):
        df["Kurulum_Tarihi"] = pd.to_datetime(df["Kurulum_Tarihi"], errors='coerce')

    df["Tref_Yas_Gun"] = (t_ref - df["Kurulum_Tarihi"]).dt.days.clip(lower=0)
    df["Tref_Ay"] = t_ref.month
    
    # --- Observability Features ---
    # Instead of np.maximum, we use Pandas .clip(lower=...) which handles Timestamps correctly
    effective_start_date = df["Kurulum_Tarihi"].clip(lower=observation_start_date)
    
    # How long have we actually watched this asset?
    df["Observed_Duration_Days"] = (t_ref - effective_start_date).dt.days
    
    # Is it a "Legacy" asset (existed before we started recording)?
    df["Is_Legacy_Asset"] = (df["Kurulum_Tarihi"] < observation_start_date).astype(int)
    
    # Ratio: What % of its life did we observe?
    # Avoid division by zero for brand new assets
    df["Observation_Ratio"] = (df["Observed_Duration_Days"] / df["Tref_Yas_Gun"].replace(0, 1)).fillna(1.0).clip(0, 1)
    
    # 2. Merge chronic features if available
    if chronic_df is not None and len(chronic_df) > 0:
        # Check if columns exist before merging to avoid duplication
        # --- DÃœZELTME BURADA: MTBF_Bayes_Gun EKLENDÄ° ---
        cols_to_merge = [
            "Ariza_Sayisi_90g", 
            "Chronic_Rate_Yillik", 
            "Chronic_Decay_Skoru", 
            "Chronic_Flag", 
            "MTBF_Bayes_Gun"  # <--- ARTIK LÄ°STEDE!
        ]
        cols_to_merge = [c for c in cols_to_merge if c in chronic_df.columns]
        
        # Drop existing columns if they are already there (to allow re-calculation)
        df.drop(columns=[c for c in cols_to_merge if c in df.columns], inplace=True, errors="ignore")
        
        # Use merge (left join)
        df = df.merge(chronic_df[["cbs_id"] + cols_to_merge], on="cbs_id", how="left")
        
        # Fill NaN for equipment with no chronic history
        df[cols_to_merge] = df[cols_to_merge].fillna(0)
    else:
        # If no chronic data, create 0 columns to prevent crashes later
        df["Ariza_Sayisi_90g"] = 0
        df["Chronic_Rate_Yillik"] = 0.0
        df["Chronic_Decay_Skoru"] = 0.0
        df["Chronic_Flag"] = 0
        df["MTBF_Bayes_Gun"] = 0  # <--- EKLENDÄ°
    
    logger.info(f"[FEATURES] Temporal: Added age + chronic features (incl. MTBF) + observability stats")
    return df
# =============================================================================
# STEP 03: MODEL TRAINING
# =============================================================================
# =============================================================================
# ğŸ§¹ MULTICOLLINEARITY CLEANER (Ã‡OKLU BAÄLANTI TEMÄ°ZLÄ°ÄÄ°)
# =============================================================================
# Bu fonksiyon, modelin stabilitesini bozan "birbirinin kopyasÄ±" deÄŸiÅŸkenleri temizler.
#
# ğŸ” Sorun (Multicollinearity):
#    - Ã–rnek: 'Tref_Yas_Gun' ile 'Kurulum_Yili' neredeyse aynÄ± bilgiyi taÅŸÄ±r.
#    - Ä°kisi birden modele girerse, Cox/Regression modellerinin katsayÄ±larÄ± (Coefficients)
#      gÃ¼venilmez hale gelir ve standart hatalar aÅŸÄ±rÄ± bÃ¼yÃ¼r.
#
# ğŸ› ï¸ Ã‡Ã¶zÃ¼m (Iterative VIF Removal):
#    1. TÃ¼m sayÄ±sal deÄŸiÅŸkenlerin VIF (Variance Inflation Factor) deÄŸerini hesaplar.
#    2. VIF deÄŸeri eÅŸiÄŸi (Genelde 10.0) geÃ§en deÄŸiÅŸkenlerden EN YÃœKSEK olanÄ± seÃ§er.
#    3. O deÄŸiÅŸkeni veri setinden atar.
#    4. Kalan deÄŸiÅŸkenlerle VIF'i tekrar hesaplar (Ã‡Ã¼nkÃ¼ birini atÄ±nca diÄŸerleri dÃ¼zelebilir).
#    5. TÃ¼m VIF deÄŸerleri < 10 olana kadar bu dÃ¶ngÃ¼ devam eder.
# =============================================================================

def remove_multicollinear_features(X: pd.DataFrame, threshold: float = 10.0, logger=None) -> pd.DataFrame:
    """
    Remove features with high VIF (Variance Inflation Factor)
    VIF > 10 indicates severe multicollinearity
    """
    from statsmodels.stats.outliers_influence import variance_inflation_factor
    
    # Only numeric columns
    X_numeric = X.select_dtypes(include=[np.number])
    
    if X_numeric.shape[1] < 2:
        return X  # Need at least 2 features for VIF
    
    # Calculate VIF iteratively
    while True:
        vif_data = pd.DataFrame()
        vif_data["Feature"] = X_numeric.columns
        vif_data["VIF"] = [variance_inflation_factor(X_numeric.values, i) 
                          for i in range(X_numeric.shape[1])]
        
        max_vif = vif_data["VIF"].max()
        
        if max_vif > threshold:
            drop_feature = vif_data.loc[vif_data["VIF"].idxmax(), "Feature"]
            if logger:
                logger.info(f"[VIF] Dropping {drop_feature} (VIF={max_vif:.1f})")
            X_numeric = X_numeric.drop(columns=[drop_feature])
            X = X.drop(columns=[drop_feature])
        else:
            break
    
    return X
# =============================================================================
# ğŸ¯ FEATURE SELECTION (Ã–ZELLÄ°K SEÃ‡Ä°MÄ° VE BOYUT Ä°NDÄ°RGEME)
# =============================================================================
# Bu fonksiyon, modelin performansÄ±nÄ± artÄ±rmak iÃ§in "En DeÄŸerli" Ã¶zellikleri seÃ§er.
#
# ğŸ” Neden YapÄ±yoruz?
#    - One-Hot Encoding sonrasÄ± (Marka, Mahalle vb.) yÃ¼zlerce sÃ¼tun oluÅŸabilir.
#    - Ã‡ok fazla sÃ¼tun (High Dimensionality), modelin gereksiz veriyi ezberlemesine
#      (Overfitting) ve eÄŸitim sÃ¼resinin uzamasÄ±na neden olur.
#
# ğŸ› ï¸ NasÄ±l Ã‡alÄ±ÅŸÄ±r?
#    1. GeÃ§ici bir Random Forest modeli eÄŸitilir.
#    2. Bu model, her bir Ã¶zelliÄŸin tahmine ne kadar katkÄ± saÄŸladÄ±ÄŸÄ±nÄ± (Feature Importance) Ã¶lÃ§er.
#    3. Sadece en yÃ¼ksek puana sahip ilk 'K' Ã¶zellik (top_k) tutulur.
#    4. Geri kalan "GÃ¼rÃ¼ltÃ¼" (Noise) niteliÄŸindeki zayÄ±f Ã¶zellikler veri setinden atÄ±lÄ±r.
# =============================================================================

def select_top_features(X_train, y_train, X_test, top_k=20, logger=None):
    """
    Select top K features using Random Forest importance
    Useful when you have many features after one-hot encoding
    """
    from sklearn.ensemble import RandomForestClassifier
    
    if X_train.shape[1] <= top_k:
        return X_train, X_test  # Already fewer than top_k
    
    # Train quick RF to get importances
    rf = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)
    #rf = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=1)
    rf.fit(X_train, y_train)
    
    # Get top K features
    importances = pd.Series(rf.feature_importances_, index=X_train.columns)
    top_features = importances.nlargest(top_k).index.tolist()
    
    if logger:
        logger.info(f"[FEATURE IMPORTANCE] Selected top {len(top_features)} features")
    
    return X_train[top_features], X_test[top_features]

# =============================================================================
# ğŸ‘¯ HIGH CORRELATION FILTER (YÃœKSEK KORELASYON FÄ°LTRESÄ°)
# =============================================================================
# Bu fonksiyon, VIF analizinden Ã¶nce yapÄ±lan "HÄ±zlÄ± ve Kaba" temizliktir.
#
# ğŸ” AmaÃ§:
#    - Birbiriyle %95'ten fazla (threshold=0.95) benzerlik gÃ¶steren deÄŸiÅŸken Ã§iftlerini bulur.
#    - Ã–rnek: "SÄ±caklÄ±k (C)" ve "SÄ±caklÄ±k (F)". Bu ikisi matematiksel olarak aynÄ± bilgidir.
#    - Ä°kisini birden modele vermek, modelin kafasÄ±nÄ± karÄ±ÅŸtÄ±rÄ±r (Multicollinearity).
#
# ğŸ› ï¸ YÃ¶ntem:
#    1. Korelasyon matrisini (Pearson) Ã§Ä±karÄ±r.
#    2. Matrisin simetrik olduÄŸunu bildiÄŸi iÃ§in sadece "Ãœst ÃœÃ§gen"e (Upper Triangle) bakar.
#    3. Ä°liÅŸkisi 0.95'i geÃ§en Ã§iftlerden ikincisini (sÃ¼tun bazÄ±nda sonra geleni) siler.
# =============================================================================
def remove_highly_correlated_features(X: pd.DataFrame, threshold=0.95, logger=None):
    """
    Remove features with correlation > threshold to another feature
    Keeps the first of each correlated pair
    """
    # Only numeric
    X_numeric = X.select_dtypes(include=[np.number])
    
    if X_numeric.shape[1] < 2:
        return X
    
    # Calculate correlation matrix
    corr_matrix = X_numeric.corr().abs()
    
    # Upper triangle (avoid duplicates)
    upper_tri = corr_matrix.where(
        np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
    )
    
    # Find features with correlation > threshold
    to_drop = [column for column in upper_tri.columns 
               if any(upper_tri[column] > threshold)]
    
    if to_drop and logger:
        logger.info(f"[CORRELATION] Dropping {len(to_drop)} highly correlated features: {to_drop}")
    
    return X.drop(columns=to_drop)


# =============================================================================
# ğŸ­ PREPROCESSING PIPELINE (VERÄ° Ã–N Ä°ÅLEME HATTI)
# =============================================================================
# Bu fonksiyon, ham veriyi modelin anlayacaÄŸÄ± matematiksel formata dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.
#
# ğŸ”¢ SayÄ±sal Veriler Ä°Ã§in:
#    - Eksik Veriler (NaN): Medyan ile doldurulur (Median Imputation).
#      Bu yÃ¶ntem, aykÄ±rÄ± deÄŸerlerin (Outliers) ortalamayÄ± bozmasÄ±nÄ± engeller.
#
# ğŸ”  Kategorik (Metin) Veriler Ä°Ã§in:
#    - Eksik Veriler: En sÄ±k geÃ§en deÄŸer (Mode) ile doldurulur.
#    - DÃ¶nÃ¼ÅŸÃ¼m: One-Hot Encoding uygulanÄ±r.
#      Ã–rn: "Marka: Siemens" -> [0, 0, 1, 0] gibi binary vektÃ¶re dÃ¶nÃ¼ÅŸÃ¼r.
#    - GÃ¼venlik: `handle_unknown='ignore'` sayesinde, gelecekte bilinmeyen
#      bir kategori gelirse sistem Ã§Ã¶kmez, sadece o Ã¶zelliÄŸi 0 sayar.
# =============================================================================
def build_preprocessor(X: pd.DataFrame, logger=None): # <--- logger parametresi eklendi
    """Sklearn pipeline for numeric + categorical features"""
    
    # 1. SayÄ±sal ve Kategorik SÃ¼tunlarÄ± Otomatik AyÄ±r
    num_cols = X.select_dtypes(include=[np.number]).columns.tolist()
    cat_cols = [c for c in X.columns if c not in num_cols]
    
    # 2. Hangi sÃ¼tun ne iÅŸlem gÃ¶recek LOG'a yaz (Casus KÄ±sÄ±m)
    if logger:
        logger.info("-" * 40)
        logger.info(f"[PREPROCESS] SayÄ±sal SÃ¼tunlar (Median Impute): {len(num_cols)} adet")
        logger.info(f"   List: {num_cols}")
        logger.info(f"[PREPROCESS] Kategorik SÃ¼tunlar (Mode Impute): {len(cat_cols)} adet")
        logger.info(f"   List: {cat_cols}")
        logger.info("-" * 40)

    # 3. Pipeline Kurulumu (DeÄŸiÅŸmedi)
    numeric_pipe = Pipeline([("imputer", SimpleImputer(strategy="median"))])
    categorical_pipe = Pipeline([
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=False)),
    ])
    
    return ColumnTransformer(
        transformers=[
            ("num", numeric_pipe, num_cols),
            ("cat", categorical_pipe, cat_cols)
        ],
        remainder="drop"
    )
# =============================================================================
# ğŸ›¡ï¸ COX MODEL SAFETY & LEAKAGE PREVENTION (COX GÃœVENLÄ°K FÄ°LTRELERÄ°)
# =============================================================================
# Bu modÃ¼l, Cox Proportional Hazards modelinin matematiksel olarak Ã§Ã¶kmesini ve
# hile yapmasÄ±nÄ± (Data Leakage) engeller.
#
# 1. select_survival_safe_features:
#    - "GeleceÄŸi GÃ¶steren" verileri temizler.
#    - Ã–rn: 'Fault_Count' veya 'Son_Ariza_Tarihi' verilirse, model varlÄ±ÄŸÄ±n ne kadar
#      yaÅŸadÄ±ÄŸÄ±nÄ± dolaylÄ± yoldan Ã¶ÄŸrenir (Leakage). Bu fonksiyon bunlarÄ± yasaklar.
#
# 2. select_cox_safe_features:
#    - Cox modelinin en bÃ¼yÃ¼k dÃ¼ÅŸmanÄ± "Singular Matrix" (Tersi alÄ±namayan matris) hatasÄ±dÄ±r.
#    - Bu hatayÄ± Ã¶nlemek iÃ§in:
#      a. Sabit DeÄŸerler (Constant Columns): Her satÄ±rda aynÄ± olan veriler atÄ±lÄ±r.
#      b. High Cardinality: 20'den fazla seÃ§eneÄŸi olan kategorik veriler (Ã¶rn. Mahalle)
#         modeli ÅŸiÅŸirmesin diye atÄ±lÄ±r.
#      c. Multicollinearity: VIF ve Korelasyon testleri ile birbirinin kopyasÄ± olan
#         deÄŸiÅŸkenler temizlenir.
# =============================================================================

def select_survival_safe_features(df: pd.DataFrame, structural_cols: list, logger: logging.Logger) -> list:
    """Filter to leakage-free features"""
    forbidden = (FEATURE_REGISTRY["temporal_leakage"] + 
                 FEATURE_REGISTRY["chronic_features"])
    
    safe_cols = [c for c in structural_cols if c in df.columns and c not in forbidden]
    logger.info(f"[FEATURE SELECT] Safe: {len(safe_cols)}/{len(structural_cols)}")
    return safe_cols


def select_cox_safe_features(df: pd.DataFrame, structural_cols: list, logger: logging.Logger) -> pd.DataFrame:
    """
    âœ… FIXED: Added VarianceThreshold to drop constant features.
    """
    from sklearn.feature_selection import VarianceThreshold

    base_cols = select_survival_safe_features(df, structural_cols, logger)
    
    # Add Kurulum_Tarihi for temporal split logic
    if "Kurulum_Tarihi" in df.columns and "Kurulum_Tarihi" not in base_cols:
        base_cols = base_cols + ["Kurulum_Tarihi"]
    
    X = df[base_cols].copy()
    
    # Preserve Kurulum_Tarihi before encoding/filtering
    kurulum_col = None
    if "Kurulum_Tarihi" in X.columns:
        kurulum_col = X["Kurulum_Tarihi"].copy()
        X = X.drop(columns=["Kurulum_Tarihi"])
    
    # One-hot encode categoricals
    cat_cols = X.select_dtypes(include="object").columns.tolist()
    for col in cat_cols[:]:
        if X[col].nunique() > 20:
            logger.warning(f"[COX] Dropping {col}: high cardinality (>20)")
            X = X.drop(columns=[col])
            cat_cols.remove(col)
    
    if cat_cols:
        X = pd.get_dummies(X, columns=cat_cols, drop_first=True, dtype=float)
    
    # Convert to numeric
    X = X.apply(pd.to_numeric, errors="coerce").fillna(0)
    # Add this inside select_cox_safe_features, before VIF check
    for col in X.columns:
        if X[col].nunique() <= 1:
            logger.info(f"[DROP] {col} is constant (single value)")
            X = X.drop(columns=[col])
    # âœ… FILTER LOW VARIANCE (Constant Columns)
    # Drop features where 99% of values are the same
    selector = VarianceThreshold(threshold=(.99 * (1 - .99)))
    # 5. âœ… Remove low variance
    #X = remove_low_variance_features(X, logger)
    
    # 6.âš ï¸ NEW: Remove highly correlated features
    X = remove_highly_correlated_features(X, threshold=0.95, logger=logger)
     
    # 7. âš ï¸ NEW: Remove multicollinear features
    #X = remove_multicollinear_features(X, threshold=10.0, logger=logger)
    X = remove_multicollinear_features(X, threshold=20.0, logger=logger)
    try:
        selector.fit(X)
        kept_cols = X.columns[selector.get_support()]
        dropped_count = len(X.columns) - len(kept_cols)
        if dropped_count > 0:
            logger.info(f"[COX] Dropped {dropped_count} low-variance columns")
        X = X[kept_cols]
    except ValueError:
        # Happens if X is empty or all constant
        pass

    # Re-add Kurulum_Tarihi at end
    if kurulum_col is not None:
        X["Kurulum_Tarihi"] = kurulum_col
    
    return X
# =============================================================================
# ğŸ§  SURVIVAL MODEL TRAINING (COX & WEIBULL EÄÄ°TÄ°MÄ°)
# =============================================================================
# Bu fonksiyon, temizlenmiÅŸ veriyi alarak iki temel Survival modelini eÄŸitir.
#
# 1. Temporal Split (Zamansal BÃ¶lme):
#    - Veriyi rastgele deÄŸil, Kurulum Tarihine gÃ¶re bÃ¶ler (Eskiler Train, Yeniler Test).
#    - Bu yÃ¶ntem, modelin "GeleceÄŸi Tahmin Etme" yeteneÄŸini daha gerÃ§ekÃ§i Ã¶lÃ§er.
#
# 2. Cox PH Model (with Left Truncation):
#    - 'entry_days' parametresi kullanÄ±larak "Delayed Entry" (Gecikmeli GiriÅŸ) tanÄ±tÄ±lÄ±r.
#    - Bu, veri toplamaya baÅŸlamadan Ã¶nce kurulmuÅŸ varlÄ±klarÄ±n yarattÄ±ÄŸÄ± "Bias"Ä± siler.
#    - Penalizer (0.1): AÅŸÄ±rÄ± Ã¶ÄŸrenmeyi (Overfitting) ve matris hatalarÄ±nÄ± Ã¶nler.
#
# 3. Weibull AFT Model:
#    - Cox modeline alternatif olarak eÄŸitilir. Parametrik yapÄ±sÄ± sayesinde bazen
#      gelecek tahminlerinde daha kararlÄ± sonuÃ§lar verebilir.
# =============================================================================
def train_cox_weibull(
    X: pd.DataFrame,
    duration: pd.Series,
    event: pd.Series,
    entry: pd.Series, # <--- NEW ARGUMENT
    logger: logging.Logger
):
    """
    âœ… FIXED: Uses .loc for splitting with Index Labels.
    """
    if not LIFELINES_OK:
        return None, None
    
    work = X.copy()
    work["duration_days"] = duration.values
    work["event"] = event.values
    work["entry_days"] = entry.values # <--- Add this
    # âœ… ADD SAFETY CHECK
    # Remove helper columns to count actual features
    feature_cols = [c for c in work.columns if c not in ["duration_days", "event", "entry_days", "Kurulum_Tarihi"]]
    
    if len(feature_cols) == 0:
        logger.warning("[COX] No features left after filtering - skipping Cox/Weibull")
        return None, None
    # Check for temporal column
    has_kurulum = "Kurulum_Tarihi" in work.columns
    if has_kurulum:
        kurulum_backup = work["Kurulum_Tarihi"].copy()
    
    # Ensure numeric matrix for Lifelines
    # (Exclude Kurulum_Tarihi from the numeric conversion step to avoid errors)
    numeric_cols = work.columns.drop(["Kurulum_Tarihi"]) if has_kurulum else work.columns
    work[numeric_cols] = work[numeric_cols].apply(pd.to_numeric, errors="coerce").fillna(0)
    
    try:
        if has_kurulum:
            # Restore date column for splitting
            work["Kurulum_Tarihi"] = kurulum_backup
            # Get LABELS
            train_labels, test_labels = temporal_train_test_split(work, test_size=0.25, logger=logger)
            
            # Use .loc because we have Labels
            train_data = work.loc[train_labels]
            test_data = work.loc[test_labels]
        else:
            raise ValueError("Kurulum_Tarihi missing")
            
    except Exception as e:
        logger.warning(f"[COX] Temporal split failed ({e}), using random split")
        # Random split on INDEX LABELS
        train_labels, test_labels = train_test_split(
            work.index.values, test_size=0.25, random_state=42, stratify=event.values
        )
        train_data = work.loc[train_labels]
        test_data = work.loc[test_labels]
    
    # Drop helper column before training
    train_data = train_data.drop(columns=["Kurulum_Tarihi"], errors="ignore")
    test_data = test_data.drop(columns=["Kurulum_Tarihi"], errors="ignore")
    
    # Cox Training
    cox = None
    # Cox Training
    try:
        #cox = CoxPHFitter(penalizer=0.05)
        cox = CoxPHFitter(penalizer=0.1)
        # TELL COX ABOUT DELAYED ENTRY
        cox.fit(
            train_data, 
            duration_col="duration_days", 
            event_col="event", 
            entry_col="entry_days" # <--- THE MAGIC FIX
        )
    except Exception as e:
        logger.error(f"[COX] Training failed: {e}")
    
    # Weibull Training
    wb = None
    try:
        #wb = WeibullAFTFitter(penalizer=0.05)
        wb = WeibullAFTFitter(penalizer=0.1)
        wb.fit(train_data, duration_col="duration_days", event_col="event")
        wb_pred = wb.predict_median(test_data)
        wb_cind = concordance_index(test_data["duration_days"], wb_pred, test_data["event"])
        logger.info(f"[WEIBULL] Test Concordance: {wb_cind:.4f}")
    except Exception as e:
        logger.error(f"[WEIBULL] Training failed: {e}")
    
    return cox, wb
# =============================================================================
# ğŸŒ² RANDOM SURVIVAL FOREST (RSF) TRAINING
# =============================================================================
# Bu fonksiyon, makine Ã¶ÄŸrenmesi tabanlÄ±, doÄŸrusal olmayan (non-linear) bir
# yaÅŸam analizi modeli eÄŸitir.
#
# ğŸ¥Š Cox Modeli vs RSF:
#    - Cox: "Marka X riski %20 artÄ±rÄ±r" gibi genel kurallar bulur. (Yorumlanabilir)
#    - RSF: "Marka X, sadece Salihli bÃ¶lgesindeyse ve yaÅŸÄ± > 10 ise risklidir" gibi
#      karmaÅŸÄ±k etkileÅŸimleri yakalar. (Daha yÃ¼ksek tahmin gÃ¼cÃ¼)
#
# ğŸ”§ Kritik MÃ¼hendislik (Indexing Fix):
#    - Pandas DataFrame (Etiket bazlÄ±) ile Scikit-Survival Array (SÄ±ra bazlÄ±)
#      arasÄ±ndaki uyumsuzluÄŸu Ã§Ã¶zmek iÃ§in 'get_indexer' kullanÄ±lÄ±r.
#      Bu sayede Temporal Split sÄ±rasÄ±nda veri kaymasÄ± yaÅŸanmaz.
# =============================================================================
def train_rsf_survival(
    df: pd.DataFrame,
    structural_cols: list,
    logger: logging.Logger
):
    """
    âœ… FIXED: Handles DataFrame (.loc) vs Numpy Array (positional) indexing correctly.
    """
    if not SKSURV_OK:
        return None
    
    cols = select_survival_safe_features(df, structural_cols, logger)
    X = df[cols].copy()
    
    # Drop columns that are completely empty/NaN to satisfy sksurv
    X = X.dropna(axis=1, how='all')
    
    # Create structured array for target
    y = Surv.from_arrays(
        event=df["event"].astype(bool).values,
        time=df["duration_days"].values
    )
    
    try:
        # Get LABELS
        train_labels, test_labels = temporal_train_test_split(df, test_size=0.25, logger=logger)
        
        # DataFrame uses .loc with labels
        X_train = X.loc[train_labels]
        X_test = X.loc[test_labels]
        
        # Numpy array needs Integer Positions -> Convert labels to positions
        train_pos = df.index.get_indexer(train_labels)
        test_pos = df.index.get_indexer(test_labels)
        
        y_train = y[train_pos]
        y_test = y[test_pos]
        
    except Exception as e:
        logger.warning(f"[RSF] Temporal split failed ({e}), using random")
        # Fallback using standard split (returns arrays directly)
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.25, random_state=42, stratify=df["event"].values
        )
    
    # Build pipeline
    pre = build_preprocessor(X_train)
    rsf = RandomSurvivalForest(
        #n_estimators=200,
        #n_estimators=100,
        n_estimators=50,
        min_samples_split=10,
        min_samples_leaf=5,
        random_state=42,
        n_jobs=-1
    )
    
    pipe = Pipeline([("pre", pre), ("rsf", rsf)])
    try:
        pipe.fit(X_train, y_train)
        
        risk = pipe.predict(X_test)
        ci = concordance_index_censored(y_test["event"], y_test["time"], risk)[0]
        logger.info(f"[RSF] Test Concordance: {ci:.4f}")
        return pipe
    except Exception as e:
        logger.error(f"[RSF] Training failed: {e}")
        return None
    

# =============================================================================
# ğŸš€ GRADIENT BOOSTING SURVIVAL ANALYSIS (GBSA)
# =============================================================================
# Bu fonksiyon, "Boosting" tekniÄŸini kullanarak bir yaÅŸam analizi modeli eÄŸitir.
#
# ğŸ”„ 1. StandartlaÅŸtÄ±rÄ±lmÄ±ÅŸ Zamansal BÃ¶lme (Consistency):
#    - Daha Ã¶nceki manuel sÄ±ralama yerine, projenin ortak fonksiyonu olan
#      'temporal_train_test_split' kullanÄ±lÄ±r.
#    - BÃ¶ylece Cox, RSF ve GBSA modelleri birebir AYNI eÄŸitim ve test verisi
#      Ã¼zerinde yarÄ±ÅŸÄ±r. SonuÃ§lar adil bir ÅŸekilde kÄ±yaslanabilir.
#
# ğŸ› ï¸ 2. Ä°ndeksleme MÃ¼hendisliÄŸi (Label vs Position):
#    - Sorun: X (DataFrame) etiket bazlÄ± (.loc), y (Numpy Array) sÄ±ra bazlÄ± Ã§alÄ±ÅŸÄ±r.
#    - Ã‡Ã¶zÃ¼m: 'get_indexer' metodu ile EÄŸitim/Test etiketleri (Labels), Numpy dizisinin
#      anlayacaÄŸÄ± sÄ±ra numaralarÄ±na (Position) Ã§evrilir. Bu, veri kaymasÄ±nÄ± Ã¶nler.
#
# ğŸ¥Š 3. GBSA vs RSF:
#    - RSF (Random Forest): Paralel Ã§alÄ±ÅŸÄ±r, karar aÄŸaÃ§larÄ±nÄ±n ortalamasÄ±nÄ± alÄ±r.
#    - GBSA (Gradient Boosting): Seri Ã§alÄ±ÅŸÄ±r. Her yeni aÄŸaÃ§, bir Ã¶nceki aÄŸacÄ±n
#      yaptÄ±ÄŸÄ± hatalarÄ± dÃ¼zeltmek iÃ§in kurulur. Genellikle daha keskin tahmin yapar.
# =============================================================================
def train_ml_models(
    df: pd.DataFrame,
    feature_cols: list,
    horizons_days: list,
    logger: logging.Logger
):
    try:
        from sksurv.ensemble import GradientBoostingSurvivalAnalysis
        from sksurv.util import Surv
        from sklearn.pipeline import Pipeline
    except ImportError:
        logger.warning("[ML] sksurv not installed. Skipping ML.")
        return None

    # 1. Temiz bir kopya oluÅŸtur
    work_df = df.copy()
    
    # 2. Sadece gÃ¼venli sÃ¼tunlarÄ± seÃ§
    X = work_df[feature_cols].copy()
    X = X.select_dtypes(include=[np.number, 'object'])
    
    # 3. Hedef DeÄŸiÅŸkeni (Target) OluÅŸtur - Structured Array
    y = Surv.from_arrays(
        event=work_df["event"].astype(bool).values,
        time=work_df["duration_days"].values
    )

    # 4. TEMPORAL SPLIT (STANDARTLAÅTIRILMIÅ) 
    # Manuel sÄ±ralama yerine ortak fonksiyonu kullanÄ±yoruz.
    try:
        # Fonksiyon bize EÄŸitim ve Test iÃ§in ID listelerini (Labels) verir
        train_labels, test_labels = temporal_train_test_split(work_df, test_size=0.25, logger=logger)
        
        # --- KRÄ°TÄ°K NOKTA: LABEL vs POSITION ---
        
        # A) X bir DataFrame'dir, doÄŸrudan Etiket (.loc) ile bÃ¶lebiliriz
        X_train = X.loc[train_labels]
        X_test = X.loc[test_labels]
        
        # B) y bir Numpy dizisidir, Etiket anlamaz, Pozisyon (SÄ±ra No) ister.
        # Bu yÃ¼zden Etiketleri -> Pozisyon NumarasÄ±na Ã§eviriyoruz.
        train_pos = work_df.index.get_indexer(train_labels)
        test_pos = work_df.index.get_indexer(test_labels)
        
        y_train = y[train_pos]
        y_test = y[test_pos]
        
    except Exception as e:
        logger.warning(f"[ML] Temporal split failed ({e}), using random split")
        # EÄŸer tarih yoksa veya hata olursa rastgele bÃ¶l
        from sklearn.model_selection import train_test_split
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.25, random_state=42, stratify=work_df["event"].values
        )

    # 5. Pipeline Kurulumu (GBSA)
    pre = build_preprocessor(X_train)
    
    gbsa = GradientBoostingSurvivalAnalysis(
        #n_estimators=100,
        n_estimators=50,
        learning_rate=0.1,
        max_depth=3,
        loss="coxph",  # Cox mantÄ±ÄŸÄ±yla optimize et
        random_state=42
    )

    model_pipeline = Pipeline([("pre", pre), ("gbsa", gbsa)])
    
    try:
        logger.info(f"[ML] Training GBSA on {len(X_train)} samples...")
        model_pipeline.fit(X_train, y_train)
        
        # Test Skoru (C-Index)
        score = model_pipeline.score(X_test, y_test)
        logger.info(f"[ML] GBSA Test Concordance: {score:.4f}")
        
        return {"model": model_pipeline, "safe_cols": feature_cols}
        
    except Exception as e:
        logger.warning(f"[ML] Training failed: {e}")
        return None

# =============================================================================
# BACKTESTING (Temporal Validation Proof)
# =============================================================================
# =============================================================================
# ğŸ•°ï¸ TEMPORAL BACKTESTER (ZAMANDA YOLCULUK TESTÄ°)
# =============================================================================
# Bu sÄ±nÄ±f, modelin geÃ§miÅŸteki performansÄ±nÄ± simÃ¼le ederek "GeleceÄŸi GÃ¶rme" (Look-ahead Bias)
# riskini test eder.
#
# ğŸ”„ Ã‡alÄ±ÅŸma MantÄ±ÄŸÄ± (Walk-Forward Validation):
#    1. Zamanda Geriye Git: Ã–rn. 1 Ocak 2022 tarihine dÃ¶n.
#    2. GeleceÄŸi Sil: 2022 sonrasÄ±ndaki tÃ¼m arÄ±zalarÄ± ve verileri yok et.
#    3. Modeli EÄŸit: Sadece 2022 Ã¶ncesi verilerle bir model kur.
#    4. Tahmin Yap: 2022 yÄ±lÄ± iÃ§inde hangi trafolarÄ±n bozulacaÄŸÄ±nÄ± tahmin et.
#    5. GerÃ§ekle KÄ±yasla: 2022'de gerÃ§ekten bozulanlarla tahminleri karÅŸÄ±laÅŸtÄ±r.
#
# ğŸ“Š Kritik Metrik (Top-100 Precision):
#    - "Modelin en riskli dediÄŸi 100 trafonun kaÃ§Ä± o yÄ±l gerÃ§ekten bozuldu?"
#    - Bu, sahadaki bakÄ±m ekipleri iÃ§in en hayati metriktir (Return on Investment).
# =============================================================================
class TemporalBacktester:
    """Enhanced backtester with proper temporal split and survival modeling"""
    
    def __init__(self, df_fault: pd.DataFrame, df_healthy: pd.DataFrame, logger: logging.Logger):
        self.df_fault = df_fault
        self.df_healthy = df_healthy
        self.logger = logger
        self.results = []
    
    def _generate_snapshot(self, cutoff_date: pd.Timestamp):
        """Create training dataset as it would have looked at cutoff_date"""
        faults_past = self.df_fault[self.df_fault["started at"] <= cutoff_date].copy()

        # Data quality already checked in load_fault_data()
        faults_filtered = faults_past.copy()

        observation_start_date = self.df_fault["started at"].min() if not self.df_fault.empty else cutoff_date
        
        equipment_master = build_equipment_master(faults_past, self.df_healthy, self.logger, cutoff_date)
        
        df_snapshot = add_survival_columns_inplace(
            equipment_master, 
            faults_filtered, 
            cutoff_date, 
            observation_start_date,
            self.logger
        )
        
        chronic_df = compute_chronic_features(faults_past, cutoff_date, self.logger)
        
        df_snapshot = add_temporal_features_inplace(
            df_snapshot, 
            cutoff_date, 
            chronic_df, 
            observation_start_date,
            self.logger
        )
        
        return df_snapshot
    
    def _train_simple_model(self, df: pd.DataFrame, features: list):
        """
        âœ… FULLY ALIGNED: Uses same preprocessor as main pipeline
        """
        # 1. Feature Selection
        X = df[features].copy()

        # 2. âœ… USE MAIN PIPELINE'S PREPROCESSOR (build_preprocessor)
        preprocessor = build_preprocessor(X, logger=self.logger)

        # 3. âœ… TEMPORAL SPLIT (Same as main pipeline)
        try:
            # Use df with metadata for temporal split
            train_labels, test_labels = temporal_train_test_split(
                df,
                test_size=0.25,
                logger=self.logger
            )

            X_train = X.loc[train_labels]
            X_test = X.loc[test_labels]
            y_train = df.loc[train_labels, 'event']
            y_test = df.loc[test_labels, 'event']

        except Exception as e:
            self.logger.warning(f"[BACKTEST] Temporal split failed: {e}, using all data")
            X_train = X
            X_test = X.head(0)  # Empty test set
            y_train = df['event']
            y_test = pd.Series(dtype=int)

        # 4. âœ… FIT PREPROCESSOR + TRANSFORM
        try:
            X_train_transformed = preprocessor.fit_transform(X_train)
            X_test_transformed = preprocessor.transform(X_test) if len(X_test) > 0 else None
        except Exception as e:
            self.logger.warning(f"[BACKTEST] Preprocessing failed: {e}")
            from sklearn.dummy import DummyClassifier
            return DummyClassifier(strategy='constant', constant=0), preprocessor

        # 5. âœ… TRAIN MODEL (Reduced complexity to prevent overfitting)
        model = XGBClassifier(
            n_estimators=20,  # Reduced from 50
            max_depth=2,      # Reduced from 3
            learning_rate=0.1,  # Added learning rate
            min_child_weight=5,  # Increased from default 1
            subsample=0.8,    # Added subsampling
            colsample_bytree=0.8,  # Added feature sampling
            reg_alpha=1.0,    # L1 regularization
            reg_lambda=1.0,   # L2 regularization
            scale_pos_weight=len(y_train[y_train==0]) / max(1, len(y_train[y_train==1])),
            eval_metric="logloss",
            random_state=42,
            n_jobs=-1
        )

        try:
            model.fit(X_train_transformed, y_train)

            # âœ… Log training performance
            if X_test_transformed is not None and len(y_test) > 0:
                test_score = model.score(X_test_transformed, y_test)
                self.logger.info(f"[BACKTEST] Training accuracy: {test_score:.3f}")

            return model, preprocessor  # âœ… Return preprocessor for prediction

        except Exception as e:
            self.logger.warning(f"[BACKTEST] Model training failed: {e}")
            from sklearn.dummy import DummyClassifier
            return DummyClassifier(strategy='constant', constant=0), preprocessor
    
    def run(self, start_year: int, end_year: int, horizon_days: int = 365):
        """Run walk-forward validation"""
        self.logger.info("="*60)
        self.logger.info(f"[BACKTEST] Walk-Forward Validation ({start_year}-{end_year})")
        self.logger.info("="*60)
        
        from sklearn.metrics import roc_auc_score, precision_score, recall_score
        
        for year in range(start_year, end_year + 1):
            cutoff_date = pd.Timestamp(f"{year}-01-01")
            test_end_date = cutoff_date + pd.Timedelta(days=horizon_days)
            
            # 1. Generate Historical Snapshot
            self.logger.info(f"\n[BACKTEST] Generating snapshot for {cutoff_date.date()}...")
            df_train = self._generate_snapshot(cutoff_date)
            
            # 2. Define Ground Truth
            future_faults = self.df_fault[
                (self.df_fault["started at"] > cutoff_date) &
                (self.df_fault["started at"] <= test_end_date)
            ]

            # Data quality already checked in load_fault_data()
            future_faults = future_faults.copy()
            failed_ids = set(future_faults["cbs_id"].unique())
            
            # Target
            y_true = df_train["cbs_id"].isin(failed_ids).astype(int)
            
            if y_true.sum() < 5:
                self.logger.warning(f"[BACKTEST] {year}: Skipped - Insufficient failures ({y_true.sum()})")
                continue
            
            # âœ… Log ground truth stats
            self.logger.info(f"[BACKTEST] Ground truth: {y_true.sum()} failures out of {len(y_true)} assets ({100*y_true.mean():.2f}%)")
            
            # 3. Train Model
            exclude_cols = ["cbs_id", "event", "duration_days", "Kurulum_Tarihi", "entry_days",
                            "Ilk_Gercek_Ariza_Tarihi", "started at", "ended at", "Ilk_Ariza_Tarihi"]
            structural_cols = [c for c in df_train.columns if c not in exclude_cols]

            model, preprocessor = self._train_simple_model(df_train, structural_cols)

            # 4. Predict
            if preprocessor is not None:
                # âœ… Use same preprocessor (no manual encoding)
                X_test = df_train[structural_cols].copy()

                try:
                    X_test_transformed = preprocessor.transform(X_test)
                    probs = model.predict_proba(X_test_transformed)[:, 1]
                except Exception as e:
                    self.logger.warning(f"[BACKTEST] Prediction failed: {e}")
                    probs = np.zeros(len(y_true))
            else:
                probs = np.zeros(len(y_true))
            
            # 5. Evaluate
            try:
                auc = roc_auc_score(y_true, probs)
            except ValueError:
                auc = 0.5

            # âœ… THRESHOLD OPTIMIZATION: Find F1-optimal threshold
            from sklearn.metrics import precision_recall_curve

            try:
                precision_curve, recall_curve, thresholds = precision_recall_curve(y_true, probs)

                # Calculate F1 score for each threshold
                # Avoid division by zero
                with np.errstate(divide='ignore', invalid='ignore'):
                    f1_scores = 2 * (precision_curve * recall_curve) / (precision_curve + recall_curve)
                    f1_scores = np.nan_to_num(f1_scores)  # Replace NaN with 0

                # Find threshold that maximizes F1
                optimal_idx = np.argmax(f1_scores)
                optimal_threshold = thresholds[optimal_idx] if optimal_idx < len(thresholds) else 0.5
                optimal_f1 = f1_scores[optimal_idx]

                self.logger.info(f"[BACKTEST] Optimal F1 threshold: {optimal_threshold:.3f} (F1={optimal_f1:.3f})")

            except Exception as e:
                self.logger.warning(f"[BACKTEST] Threshold optimization failed: {e}, using percentile")
                optimal_threshold = np.percentile(probs, 95)  # Fallback to top 5%
                optimal_f1 = 0.0

            # Apply optimal threshold
            y_pred = (probs >= optimal_threshold).astype(int)

            precision = precision_score(y_true, y_pred, zero_division=0)
            recall = recall_score(y_true, y_pred, zero_division=0)
            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
            
            # Top-100 Precision
            if len(probs) >= 100:
                top_100_idx = np.argsort(probs)[-100:]
                hits = y_true.iloc[top_100_idx].sum()
                top_100_precision = hits / 100
            else:
                hits = y_true.sum()
                top_100_precision = hits / len(y_true)
            
            self.logger.info(f"[BACKTEST] {year} Results:")
            self.logger.info(f"  - AUC: {auc:.3f}")
            self.logger.info(f"  - F1 Score (Optimal): {f1:.3f}")
            self.logger.info(f"  - Precision@Optimal: {precision:.3f}")
            self.logger.info(f"  - Recall@Optimal: {recall:.3f}")
            self.logger.info(f"  - Top-100 Hits: {hits}/{min(100, len(y_true))} ({100*top_100_precision:.1f}%)")

            self.results.append({
                "Year": year,
                "AUC": auc,
                "F1_Score": f1,
                "Optimal_Threshold": optimal_threshold,
                "Precision": precision,
                "Recall": recall,
                "Top100_Hits": hits,
                "Top100_Precision": top_100_precision,
                "Total_Failures": y_true.sum(),
                "Total_Assets": len(y_true)
            })
        
        # Save Summary
        results_df = pd.DataFrame(self.results)
        if not results_df.empty:
            out_dir = os.path.dirname(self.logger.handlers[0].baseFilename).replace("loglar", "data/sonuclar")
            out_path = os.path.join(out_dir, "backtest_results_temporal.csv")
            results_df.to_csv(out_path, index=False)
            self.logger.info(f"\n[BACKTEST] Results saved: {out_path}")

            # âœ… BONUS: Calculate and log threshold stability
            if "Optimal_Threshold" in results_df.columns:
                mean_threshold = results_df["Optimal_Threshold"].mean()
                std_threshold = results_df["Optimal_Threshold"].std()
                self.logger.info(f"\n[THRESHOLD STABILITY] Mean={mean_threshold:.3f}, Std={std_threshold:.3f}")

                if std_threshold < 0.05:
                    self.logger.info("  âœ… Threshold is STABLE across years (low variance)")
                else:
                    self.logger.warning("  âš ï¸ Threshold is UNSTABLE (consider using year-specific thresholds)")

        return results_df

# =============================================================================
# EQUIPMENT-SPECIFIC MODELING
# =============================================================================
# =============================================================================
# ğŸ“Š EQUIPMENT STATISTICS (EKÄ°PMAN BAZLI VERÄ° ENVANTERÄ°)
# =============================================================================
# Bu fonksiyon, her bir ekipman tipi (Trafo, HÃ¼cre, Kesici vb.) iÃ§in veri
# yeterliliÄŸini analiz eder.
#
# ğŸ” Neyi Kontrol Eder?
#    1. Ã–rneklem Boyutu (n_total): Model kurmak iÃ§in yeterli sayÄ± var mÄ±?
#    2. ArÄ±za SayÄ±sÄ± (n_events): Modelin "Ã–lÃ¼mÃ¼" Ã¶ÄŸrenmesi iÃ§in yeterince
#       Ã¶rnek olay (Failure Event) gerÃ§ekleÅŸmiÅŸ mi?
#    3. Veri Kalitesi (has_marka): Kritik Ã¶zniteliklerin (Marka vb.) doluluk oranÄ± nedir?
#
# âš ï¸ Karar Destek:
#    - EÄŸer bir ekipman tipinde 'n_events < 5' ise, o ekipman iÃ§in Ã¶zel model
#      eÄŸitmek yerine genel model kullanmak veya o tipi analizden Ã§Ä±karmak gerekir.
# =============================================================================
# =============================================================================
# ğŸ“Š FINAL DATA AUDIT (EÄÄ°TÄ°M Ã–NCESÄ° TAM KONTROL)
# =============================================================================
# Bu fonksiyon, pipeline'Ä±n en sonunda Ã§alÄ±ÅŸarak modelin ihtiyaÃ§ duyduÄŸu TÃœM verilerin
# (hem ham hem hesaplanmÄ±ÅŸ) hazÄ±r olup olmadÄ±ÄŸÄ±nÄ± denetler.
#
# ğŸ” Kritik Kontroller:
#    1. Lat/Long: Haritalama ve mekansal analiz iÃ§in ikisinin de %100'e yakÄ±n olmasÄ± gerekir.
#    2. Durat (Duration Days): SaÄŸkalÄ±m sÃ¼resi. EÄŸer bu oran dÃ¼ÅŸÃ¼kse, tarih verilerinde
#       veya 'add_survival_columns' fonksiyonunda mantÄ±k hatasÄ± var demektir.
#    3. CalcAge (YaÅŸ): Sadece dolu olmasÄ± yetmez, >0 olmasÄ± gerekir.
#    4. MTBF: Ä°statistiksel Ã¶zelliklerin (Bayesian) hesaplanÄ±p hesaplanmadÄ±ÄŸÄ±nÄ± gÃ¶sterir.
#
# âš ï¸ Karar MekanizmasÄ±:
#    - EÄŸer 'CalcAge' veya 'Durat' %90'Ä±n altÄ±ndaysa, model Ã‡ALIÅMAZ veya hatalÄ± Ã§alÄ±ÅŸÄ±r.
#    - EÄŸer 'Lat/Long' dÃ¼ÅŸÃ¼kse sadece haritalar etkilenir, model Ã§alÄ±ÅŸmaya devam eder.
# =============================================================================
def get_equipment_stats(df: pd.DataFrame, equipment_master: pd.DataFrame, logger: logging.Logger) -> dict:
    """
    Final Audit: Checks Raw Data, Location, and Engineered Features completely.
    Returns dictionary with counts AND percentages.
    
    âœ… DÃœZELTME: has_marka ve has_bakim eklendi
    """
    stats = {}
    
    # 1. DENETÄ°M HARÄ°TASI
    audit_map = {
        "Marka": "Marka",
        "Latitude": "Lat",
        "Longitude": "Long",
        "Gerilim_Seviyesi": "Volt",
        "Bakim_Sayisi": "Maint",
        "duration_days": "Durat",
        "entry_days": "Entry",
        "Tref_Yas_Gun": "CalcAge",
        "MTBF_Bayes_Gun": "MTBF",
        "Observation_Ratio": "ObsRate"
    }

    # Log BaÅŸlÄ±klarÄ±
    headers = ["Type", "Total", "Events", "Rate"] + [v for v in audit_map.values()]
    header_fmt = "{:<15} | {:<6} | {:<6} | {:<6} | " + " | ".join([f"{{:<7}}" for _ in audit_map])
    
    logger.info("="*130)
    logger.info("[FINAL DATA AUDIT] EÄŸitim Ã–ncesi Tam Kontrol")
    logger.info(header_fmt.format(*headers))
    logger.info("-" * 130)

    for eq_type in df["Ekipman_Tipi"].unique():
        df_eq = df[df["Ekipman_Tipi"] == eq_type]
        n_total = len(df_eq)
        
        if n_total == 0: 
            continue

        # Temel metrikler
        n_events = int(df_eq["event"].sum())
        event_rate = float(df_eq["event"].mean())
        
        # âœ… MARKA sayÄ±sÄ± (absolute count)
        has_marka = 0
        if "Marka" in df_eq.columns:
            has_marka = int(df_eq["Marka"].notna().sum())

        # âœ… BAKIM sayÄ±sÄ± (absolute count)
        has_bakim = 0
        if "Bakim_Sayisi" in df_eq.columns:
            # Not null AND not zero (gerÃ§ekten bakÄ±m yapÄ±lmÄ±ÅŸ)
            has_bakim = int((df_eq["Bakim_Sayisi"].notna() & (df_eq["Bakim_Sayisi"] > 0)).sum())

        type_stats = {
            "n_total": n_total,
            "n_events": n_events,
            "event_rate": event_rate,
            "has_marka": has_marka,   # âœ… EKLENDÄ°
            "has_bakim": has_bakim,   # âœ… EKLENDÄ°
        }

        # Log satÄ±rÄ±
        row_data = [
            eq_type,
            str(n_total),
            str(n_events),
            f"{100*event_rate:.1f}%"
        ]
        
        # DetaylÄ± SÃ¼tun Kontrolleri
        for col_name, label in audit_map.items():
            val_str = "MISS" 
            pct = 0.0
            
            if col_name in df_eq.columns:
                valid_mask = df_eq[col_name].notna()
                
                # MantÄ±k KontrolÃ¼
                if col_name in ["Tref_Yas_Gun", "duration_days"]:
                    valid_mask = valid_mask & (df_eq[col_name] > 0)
                elif col_name == "Bakim_Sayisi":
                    # BakÄ±m iÃ§in: Not null (veri var demek, 0 bile bilgidir)
                    pass  # valid_mask zaten notna()
                
                valid_count = valid_mask.sum()
                pct = 100 * valid_count / n_total
                val_str = f"{pct:.0f}%"
            
            row_data.append(val_str)
            type_stats[label] = pct

        # SatÄ±rÄ± YazdÄ±r
        logger.info(header_fmt.format(*row_data))
        
        # Ana sÃ¶zlÃ¼ÄŸe kaydet
        stats[eq_type] = type_stats

    logger.info("="*130)
    
    # âœ… DEBUG LOG
    logger.info("\n[ANALYSIS ELIGIBILITY CHECK]")
    for eq_type, stat in stats.items():
        marka_ok = "âœ…" if stat['has_marka'] >= 30 else "âŒ"
        bakim_ok = "âœ…" if stat['has_bakim'] >= 30 else "âŒ"
        logger.info(
            f"  {eq_type:<15}: "
            f"Marka={stat['has_marka']:>4} {marka_ok} | "
            f"Bakim={stat['has_bakim']:>4} {bakim_ok}"
        )
    
    return stats
# =============================================================================
# STEP 04: PREDICTION
# =============================================================================
def predict_survival_pof(model, X: pd.DataFrame, duration: pd.Series, horizons: list, model_name: str, cbs_ids: pd.Series) -> pd.DataFrame:
    """Predict conditional PoF from survival model"""
    X_clean = X.drop(columns=["Kurulum_Tarihi"], errors="ignore").apply(pd.to_numeric, errors="coerce").fillna(0)
    age = duration.fillna(0).clip(lower=0).values
    
    out = pd.DataFrame({"cbs_id": cbs_ids.values})
    for H in horizons:
        label = SURVIVAL_HORIZON_LABELS.get(H, f"{H}g")
        times = np.unique(np.concatenate([age, age + H]))
        sf = model.predict_survival_function(X_clean, times=times)
        
        S_age = np.array([sf.iloc[np.searchsorted(times, a), j] for j, a in enumerate(age)])
        S_age_h = np.array([sf.iloc[np.searchsorted(times, a+H), j] for j, a in enumerate(age)])
        
        pof = 1.0 - np.clip((S_age_h + 1e-12) / (S_age + 1e-12), 0, 1)
        out[f"{model_name}_pof_{label}"] = pof
    
    return out

def predict_rsf_pof(df: pd.DataFrame, rsf_pipe, structural_cols: list, horizons: list) -> pd.DataFrame:
    """Predict conditional PoF from RSF model"""
    cols = [c for c in structural_cols if c in df.columns]
    X = df[cols].copy()
    age = df["duration_days"].fillna(0).clip(lower=0).values
    
    X_tr = rsf_pipe.named_steps["pre"].transform(X)
    sfs = rsf_pipe.named_steps["rsf"].predict_survival_function(X_tr, return_array=False)
    
    out = pd.DataFrame({"cbs_id": df["cbs_id"].values})
    for H in horizons:
        label = SURVIVAL_HORIZON_LABELS.get(H, f"{H}g")
        pofs = []
        for i, sf in enumerate(sfs):
            t, s = sf.x, sf.y
            a, b = age[i], age[i] + H
            S_a = np.interp(a, t, s, left=s[0], right=s[-1])
            S_b = np.interp(b, t, s, left=s[0], right=s[-1])
            pofs.append(1.0 - np.clip((S_b + 1e-12) / (S_a + 1e-12), 0, 1))
        out[f"rsf_pof_{label}"] = pofs
    return out
# =============================================================================
# ğŸ”® ML PREDICTION: CONDITIONAL PROBABILITY OF FAILURE (KOÅULLU RÄ°SK HESABI)
# =============================================================================
# Bu fonksiyon, eÄŸitilen modelin Ã¼rettiÄŸi "SaÄŸkalÄ±m EÄŸrilerini" (Survival Functions)
# kullanarak, her bir varlÄ±ÄŸÄ±n gelecekteki arÄ±za ihtimalini hesaplar.
#
# ğŸ§  Kritik MantÄ±k (Conditional Probability):
#    - Soru: "Bu trafo Ã¶nÃ¼mÃ¼zdeki 1 yÄ±l iÃ§inde bozulur mu?"
#    - YanlÄ±ÅŸ YÃ¶ntem: Sadece S(t=1 yÄ±l) deÄŸerine bakmak.
#    - DoÄŸru YÃ¶ntem: VarlÄ±ÄŸÄ±n ÅU ANKÄ° YAÅINI (t) hesaba katmak.
#
# ğŸ“ FormÃ¼l:
#    Risk = 1 - ( S(t + Horizon) / S(t) )
#
#    - S(t): VarlÄ±ÄŸÄ±n bugÃ¼ne kadar hayatta kalma olasÄ±lÄ±ÄŸÄ±.
#    - S(t + Horizon): Gelecekteki hedef tarihe kadar hayatta kalma olasÄ±lÄ±ÄŸÄ±.
#    - Bu formÃ¼l, "BugÃ¼ne kadar saÄŸ kalan bir varlÄ±ÄŸÄ±n, X gÃ¼n daha yaÅŸama ihtimali nedir?"
#      sorusunun cevabÄ±dÄ±r. Eski varlÄ±klar iÃ§in riski abartmayÄ± Ã¶nler.
# =============================================================================

def predict_ml_pof(df: pd.DataFrame, ml_pack: dict, horizons: list) -> pd.DataFrame:
    """
    Predicts PoF using the Survival Curves from Gradient Boosting.
    """
    # âœ… Step 1: Get the pipeline
    if "model" in ml_pack:
        pipeline = ml_pack["model"]
    elif "models" in ml_pack:
        pipeline = ml_pack["models"]
    else:
        return pd.DataFrame({"cbs_id": df["cbs_id"].values})
    
    cols = ml_pack["safe_cols"]
    X = df[cols].copy()
    current_age = df["duration_days"].fillna(0).clip(lower=0).values
    
    out = pd.DataFrame({"cbs_id": df["cbs_id"].values})
    
    # âœ… Step 2: Extract the GBSA model from the pipeline
    # Pipeline structure: [("pre", preprocessor), ("gbsa", GradientBoostingSurvivalAnalysis)]
    try:
        gbsa_model = pipeline.named_steps["gbsa"]
    except (AttributeError, KeyError):
        # Not a pipeline or no 'gbsa' step - return empty predictions
        return out
    
    # âœ… Step 3: Preprocess features
    X_transformed = pipeline.named_steps["pre"].transform(X)
    
    # âœ… Step 4: Get survival functions from GBSA
    surv_funcs = gbsa_model.predict_survival_function(X_transformed)
    
    # âœ… Step 5: Calculate conditional PoF for each horizon
    for H in horizons:
        label = SURVIVAL_HORIZON_LABELS.get(H, f"{H}g")
        pofs = []
        
        for i, fn in enumerate(surv_funcs):
            # MODELÄ°N SINIRLARINI KONTROL ET (FIX)
            max_model_time = fn.x[-1]  # Modelin bildiÄŸi en son zaman
            min_model_time = fn.x[0]   # Modelin bildiÄŸi ilk zaman

            # Mevcut yaÅŸÄ± sÄ±nÄ±rlar iÃ§ine Ã§ek
            age_now = current_age[i]
            # EÄŸer varlÄ±k modelin gÃ¶rdÃ¼ÄŸÃ¼ max yaÅŸtan bÃ¼yÃ¼kse, max yaÅŸ kabul et
            if age_now > max_model_time:
                age_now = max_model_time
            
            # Gelecek yaÅŸÄ± sÄ±nÄ±rlar iÃ§ine Ã§ek
            age_future = age_now + H
            if age_future > max_model_time:
                age_future = max_model_time

            # OlasÄ±lÄ±klarÄ± al
            # fn(t) fonksiyonu StepFunction'dÄ±r, sÄ±nÄ±r dÄ±ÅŸÄ± deÄŸerde hata verir
            try:
                prob_survive_now = fn(age_now)
                prob_survive_future = fn(age_future)
            except ValueError:
                # Hala hata alÄ±rsak (Ã§ok nadir), gÃ¼venli moda geÃ§
                prob_survive_now = 1.0
                prob_survive_future = 1.0

            # Hesaplama (SÄ±fÄ±ra bÃ¶lÃ¼nme korumasÄ±)
            if prob_survive_now < 1e-5:
                conditional_risk = 1.0 # Zaten Ã¶lÃ¼ kabul et
            else:
                conditional_risk = 1.0 - (prob_survive_future / prob_survive_now)
            
            pofs.append(np.clip(conditional_risk, 0, 1))

        out[f"ml_pof_{label}"] = pofs
        
    return out
def train_equipment_specific_models(
    df_eq: pd.DataFrame,
    structural_cols: list,
    temporal_cols: list,
    eq_type: str,
    logger: logging.Logger
) -> pd.DataFrame:
    """Train models for specific equipment type"""
    
    predictions = pd.DataFrame({"cbs_id": df_eq["cbs_id"]})
    
    # ---------------------------------------------------------
    # 1. Survival Models (Cox PH & Weibull)
    # ---------------------------------------------------------
    try:
        X_cox = select_cox_safe_features(df_eq, structural_cols, logger)
        
        # âœ… FIX: Check feature count BEFORE training
        feature_count = X_cox.shape[1] if X_cox is not None else 0
        logger.info(f"[{eq_type}] Features after filtering: {feature_count}")
        
        if feature_count < 2:
            logger.warning(f"[{eq_type}] Too few features ({feature_count}) - skipping Cox/Weibull")
        else:
            # âœ… Only train if we have enough features
            cox, wb = train_cox_weibull(
                X_cox, 
                df_eq["duration_days"], 
                df_eq["event"], 
                df_eq["entry_days"], 
                logger
            )
            
            if cox:
                cox_pred = predict_survival_pof(
                    cox, 
                    X_cox, 
                    df_eq["duration_days"], 
                    SURVIVAL_HORIZONS_DAYS, 
                    "cox", 
                    df_eq["cbs_id"]
                )
                predictions = predictions.merge(cox_pred, on="cbs_id", how="left")
                
    except Exception as e:
        logger.warning(f"[{eq_type}] Cox/Weibull failed: {e}")
    # ---------------------------------------------------------
    # 2. Random Survival Forests (RSF)
    # ---------------------------------------------------------
    try:
        rsf = train_rsf_survival(df_eq, structural_cols, logger)
        if rsf:
            rsf_pred = predict_rsf_pof(df_eq, rsf, structural_cols, SURVIVAL_HORIZONS_DAYS)
            predictions = predictions.merge(rsf_pred, on="cbs_id", how="left")
    except Exception as e:
        logger.warning(f"[{eq_type}] RSF failed: {e}")
    
    # ---------------------------------------------------------
    # 3. Machine Learning (Gradient Boosting Survival)
    # ---------------------------------------------------------
    # Note: Using Gradient Boosting Survival Analysis (GBSA), not binary classification
    ml_features = structural_cols + [c for c in temporal_cols if c not in ["Kurulum_Tarihi"]]
    
    # Check if we have enough events to learn anything useful
    n_events = df_eq["event"].sum()
    
    if n_events >= 20:  # Lowered threshold for GBSA (it learns from censored data too)
        try:
            ml_pack = train_ml_models(df_eq, ml_features, SURVIVAL_HORIZONS_DAYS, logger)
            if ml_pack:
                ml_pred = predict_ml_pof(df_eq, ml_pack, SURVIVAL_HORIZONS_DAYS)
                predictions = predictions.merge(ml_pred, on="cbs_id", how="left")
        except Exception as e:
            logger.warning(f"[{eq_type}] ML failed: {e}")
    else:
        logger.info(f"[{eq_type}] ML skipped: insufficient events ({n_events} < 20)")
    
    return predictions
# =============================================================================
# ğŸ“ˆ EXPLORATORY ANALYSIS (BETÄ°MSEL Ä°STATÄ°STÄ°KLER)
# =============================================================================
# Bu fonksiyonlar, tahmin (prediction) yapmaz; verinin rÃ¶ntgenini Ã§eker.
#
# 1. Marka Analizi:
#    - "Relative Risk" (GÃ¶receli Risk) metriÄŸi kullanÄ±lÄ±r.
#    - EÄŸer bir markanÄ±n riski 1.5 ise, ortalamadan %50 daha sÄ±k bozuluyor demektir.
#    - DÄ°KKAT: Bazen eski markalar daha sÄ±k bozulur. "Median_Age" kontrol edilmelidir.
#
# 2. BakÄ±m Etkisi:
#    - BakÄ±m sayÄ±sÄ± ile arÄ±za oranÄ± arasÄ±ndaki iliÅŸkiyi gÃ¶sterir.
#    - Beklenti: BakÄ±m arttÄ±kÃ§a arÄ±za oranÄ±nÄ±n dÃ¼ÅŸmesidir.
#    - Anomali: Bazen Ã§ok bakÄ±m yapÄ±lanlar daha Ã§ok bozulur (Reaktif BakÄ±m - ArÄ±za oldukÃ§a gitme).
# =============================================================================
def analyze_marka_effect(df_eq: pd.DataFrame, eq_type: str, logger: logging.Logger) -> pd.DataFrame:
    """
    Brand risk analysis (Marka Performans Karnesi)
    """
    if "Marka" not in df_eq.columns:
        return pd.DataFrame()

    df_marka = df_eq[df_eq["Marka"].notna()].copy()
    
    # Ä°statistiksel anlamlÄ±lÄ±k iÃ§in en az 30 veri
    if len(df_marka) < 30:
        return pd.DataFrame()
    
    # Genel OrtalamayÄ± Hesapla (KÄ±yaslama iÃ§in)
    avg_failure_rate = df_marka["event"].mean()

    marka_stats = df_marka.groupby("Marka").agg(
        Failures=("event", "sum"),
        Total=("event", "count"),
        Failure_Rate=("event", "mean"),
        Median_Age=("duration_days", "median")
    ).reset_index()
    
    # Sadece anlamlÄ± sayÄ±daki markalarÄ± al (En az 5 trafosu olan markalar)
    marka_stats = marka_stats[marka_stats["Total"] >= 5].sort_values("Failure_Rate", ascending=False)
    
    # GÃ¶receli Risk: (Marka ArÄ±za OranÄ± / Ortalama ArÄ±za OranÄ±)
    # 1.0 = Ortalama, >1.0 = Riskli, <1.0 = SaÄŸlam
    if avg_failure_rate > 0:
        marka_stats["Relative_Risk"] = marka_stats["Failure_Rate"] / avg_failure_rate
    else:
        marka_stats["Relative_Risk"] = 0.0

    logger.info(f"[{eq_type}] MARKA ANALÄ°ZÄ°: {len(marka_stats)} marka incelendi (Ort. ArÄ±za: {avg_failure_rate:.1%})")
    
    # En kÃ¶tÃ¼ 3 markayÄ± raporla
    for _, row in marka_stats.head(3).iterrows():
        logger.info(
            f"  ğŸš¨ {row['Marka']:<10} : ArÄ±za %{100*row['Failure_Rate']:.1f} | "
            f"Risk x{row['Relative_Risk']:.1f} | "
            f"YaÅŸ: {row['Median_Age']:.0f} gÃ¼n | "
            f"(N={int(row['Total'])})"
        )
    
    marka_stats["Ekipman_Tipi"] = eq_type
    return marka_stats


def analyze_bakim_effect(df_eq: pd.DataFrame, eq_type: str, logger: logging.Logger) -> pd.DataFrame:
    """
    Maintenance effect analysis (BakÄ±m Etki Analizi)
    âš ï¸ DÃœZELTME: equipment_master yerine df_eq kullanÄ±lmalÄ± (event verisi iÃ§in)
    """
    if "Bakim_Sayisi" not in df_eq.columns:
        return pd.DataFrame()
    
    # BakÄ±m sayÄ±sÄ± 0 veya daha bÃ¼yÃ¼k olanlarÄ± al (NaN'larÄ± at)
    df_bakim = df_eq[df_eq["Bakim_Sayisi"].notna()].copy()
    
    if len(df_bakim) < 30:
        return pd.DataFrame()
    
    # BakÄ±m sayÄ±larÄ±nÄ± grupla (Binning)
    # [0-1), [1-3), [3-5), [5-10), [10+)
    df_bakim["Bakim_Bin"] = pd.cut(
        df_bakim["Bakim_Sayisi"],
        bins=[-1, 0, 2, 5, 10, 1000], # -1 dahil ederek 0'Ä± yakalarÄ±z
        labels=["0 (HiÃ§)", "1-2", "3-5", "6-10", "10+"]
    )
    
    # Hangi grupta ne kadar arÄ±za var?
    bakim_stats = df_bakim.groupby("Bakim_Bin", observed=False).agg(
        Asset_Count=("cbs_id", "count"),
        Event_Count=("event", "sum"),     # <--- EKLENDÄ°
        Failure_Rate=("event", "mean")    # <--- EKLENDÄ° (Kritik Metrik)
    ).reset_index()
    
    logger.info(f"[{eq_type}] BAKIM ETKÄ°SÄ°:")
    
    # SonuÃ§larÄ± yazdÄ±r
    for _, row in bakim_stats.iterrows():
        if row['Asset_Count'] > 0:
            logger.info(
                f"  ğŸ”§ BakÄ±m {row['Bakim_Bin']:<8}: "
                f"ArÄ±za %{100*row['Failure_Rate']:.1f} "
                f"(N={row['Asset_Count']})"
            )
    
    bakim_stats["Ekipman_Tipi"] = eq_type
    return bakim_stats

# =============================================================================
# ğŸ¥ HEALTH SCORE & RISK MATRIX (SAÄLIK VE RÄ°SK PUANLAMASI)
# =============================================================================
# Bu fonksiyon, model Ã§Ä±ktÄ±sÄ±nÄ± (PoF â€“ Probability of Failure) operasyonel olarak
# anlamlÄ± bir SaÄŸlÄ±k Skoru'na (0â€“100) ve Risk SÄ±nÄ±fÄ±'na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.
#
# ğŸ“Š YÃ¶ntem: Percentile Ranking (YÃ¼zdelik SÄ±ralama â€“ GÃ¶reli Risk)
# -----------------------------------------------------------------------------
# - Mutlak olasÄ±lÄ±klar (PoF) genellikle Ã§ok kÃ¼Ã§Ã¼ktÃ¼r (Ã¶rn. %0.1 â€“ %1).
# - Bu nedenle sistem, "mutlak risk" yerine "gÃ¶reli risk" yaklaÅŸÄ±mÄ±nÄ± kullanÄ±r.
#
#   âŒ "Bu trafonun arÄ±za ihtimali %0.12"
#   âœ… "Bu trafo, aynÄ± tipteki varlÄ±klarÄ±n %95â€™inden daha risklidir"
#
# - SÄ±ralama, her ekipman tipi kendi iÃ§inde yapÄ±lÄ±r
#   (Trafo trafoyla, direk direkle kÄ±yaslanÄ±r).
#
# ğŸ§® SaÄŸlÄ±k Skoru HesabÄ±:
# -----------------------------------------------------------------------------
# - Risk sÄ±ralamasÄ± (percentile) ters Ã§evrilerek saÄŸlÄ±k skoruna dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r:
#
#     Health_Score = 100 Ã— (1 âˆ’ Risk_Percentile)
#
# - En riskli varlÄ±klar â†’ dÃ¼ÅŸÃ¼k saÄŸlÄ±k skoru
# - En saÄŸlÄ±klÄ± varlÄ±klar â†’ yÃ¼ksek saÄŸlÄ±k skoru
#
# ğŸš¦ Risk SÄ±nÄ±flarÄ± (Percentile BazlÄ±):
# -----------------------------------------------------------------------------
# - KRÄ°TÄ°K  : En kÃ¶tÃ¼ %5  (Risk_Percentile â‰¥ 0.95)
# - YÃœKSEK  : Sonraki %15 (0.80 â‰¤ Risk_Percentile < 0.95)
# - ORTA    : Sonraki %30 (0.50 â‰¤ Risk_Percentile < 0.80)
# - DÃœÅÃœK   : En iyi %50  (Risk_Percentile < 0.50)
#
# âš ï¸ Kronik VarlÄ±k KuralÄ± (Hard Business Rule):
# -----------------------------------------------------------------------------
# - "Chronic_Flag" = 1 olan varlÄ±klar, model skorundan baÄŸÄ±msÄ±z olarak
#   her zaman Ã¶ncelikli kabul edilir.
# - Bu varlÄ±klarÄ±n saÄŸlÄ±k skoru en fazla 60 ile sÄ±nÄ±rlandÄ±rÄ±lÄ±r ve
#   risk sÄ±nÄ±fÄ± otomatik olarak "KRÄ°TÄ°K (KRONÄ°K)" olarak atanÄ±r.
#
# ğŸ¯ AmaÃ§:
# -----------------------------------------------------------------------------
# - Mutlaka filonun en riskli varlÄ±klarÄ±nÄ± gÃ¶rÃ¼nÃ¼r kÄ±lmak
# - Saha ekiplerine her zaman "Ã¶ncelikli bakÄ±m listesi" Ã¼retebilmek
# - YÃ¶netim iÃ§in gÃ¶reli, karÅŸÄ±laÅŸtÄ±rÄ±labilir ve aksiyon alÄ±nabilir bir Ã§Ä±ktÄ± saÄŸlamak
# =============================================================================

# REMOVED: Duplicate/broken versions of compute_health_score
# Using only the production-grade version below

def compute_health_score(
    df: pd.DataFrame,
    pof_col: str = "PoF_Ensemble_12Ay",
    group_col: str = "Ekipman_Tipi",
    chronic_col: str = "Chronic_Flag",
    min_group_size: int = 100,
    min_health: int = 10,
    logger: logging.Logger = None,
    use_global_only: bool = True  # âœ… NEW: Force pure global ranking to fix PoF-Risk inconsistency
) -> pd.DataFrame:
    """
    Production-grade Health Score & Risk Class computation.

    âœ… FIXED: Now uses PURE GLOBAL RANKING by default to ensure consistent PoF-to-Risk mapping.
    This eliminates the issue where lower PoF values had higher risk classifications than higher PoF values.

    Args:
        use_global_only: If True (default), uses only global ranking for consistent PoF-Risk mapping.
                        If False, uses hybrid local+global ranking (may cause inconsistencies).
    """

    # --- 0. Guardrails ---
    if pof_col not in df.columns:
        df["Health_Score"] = 90
        df["Risk_Sinifi"] = "BILINMIYOR"
        return df

    df[pof_col] = df[pof_col].fillna(0).clip(0, 1)

    # --- 1. Ranking Strategy ---
    df["Global_Rank"] = df[pof_col].rank(pct=True)

    if use_global_only:
        # âœ… PURE GLOBAL RANKING: Ensures PoF-Risk consistency
        # Lower PoF will ALWAYS have lower/equal risk than higher PoF
        df["Final_Rank"] = df["Global_Rank"]
        if logger:
            logger.info("Using PURE GLOBAL ranking (PoF-Risk consistent)")
    else:
        # Hybrid ranking (old method - may cause inconsistencies)
        if group_col in df.columns:
            df["Local_Rank"] = df.groupby(group_col)[pof_col].rank(pct=True)
            group_sizes = df.groupby(group_col)[pof_col].transform("count")
            w_local = np.where(group_sizes >= min_group_size, 1.0, 0.5)
            df["Final_Rank"] = (
                df["Local_Rank"] * w_local +
                df["Global_Rank"] * (1 - w_local)
            )
            if logger:
                logger.info("Using HYBRID ranking (local+global)")
        else:
            df["Final_Rank"] = df["Global_Rank"]

    # --- 2. Health Score ---
    df["Health_Score"] = 100 * (1 - df["Final_Rank"])
    df["Health_Score"] = df["Health_Score"].clip(lower=min_health, upper=100)

    # --- 3. Chronic override ---
    if chronic_col in df.columns:
        kronik_mask = df[chronic_col] == 1
        df.loc[kronik_mask, "Health_Score"] = np.minimum(
            df.loc[kronik_mask, "Health_Score"], 60
        )

    # --- 4. Risk Class Assignment ---
    def assign_risk(row):
        if row.get(chronic_col, 0) == 1:
            return "KRÄ°TÄ°K (KRONÄ°K)"

        p = row["Final_Rank"]
        # âœ… Industry-standard thresholds (IEEE/CIGRE)
        if p >= 0.95:  # Top 5%
            return "KRÄ°TÄ°K"
        if p >= 0.85:  # Top 15%
            return "YÃœKSEK"
        if p >= 0.50:  # Top 50%
            return "ORTA"
        return "DÃœÅÃœK"  # Bottom 50%

    df["Risk_Sinifi"] = df.apply(assign_risk, axis=1)

    # --- 5. Logging ---
    if logger:
        logger.info(
            f"Risk Distribution â†’ "
            f"KRÄ°TÄ°K={sum(df['Risk_Sinifi']=='KRÄ°TÄ°K')}, "
            f"YÃœKSEK={sum(df['Risk_Sinifi']=='YÃœKSEK')}, "
            f"ORTA={sum(df['Risk_Sinifi']=='ORTA')}, "
            f"DÃœÅÃœK={sum(df['Risk_Sinifi']=='DÃœÅÃœK')}"
        )

    return df

# =============================================================================
# MAIN PIPELINE
# =============================================================================
# =============================================================================
# ğŸš€ MAIN PIPELINE ORCHESTRATION (ANA YÃ–NETÄ°M MERKEZÄ°)
# =============================================================================
# Bu fonksiyon, ham veriden nihai tahminlere giden uÃ§tan uca (End-to-End) akÄ±ÅŸÄ± yÃ¶netir.
# "PoF v4.1" mimarisinin kalbidir.
#
# ğŸ”„ Ä°ÅLEM AKIÅI (WORKFLOW):
#
# 1. ğŸ“¥ Data Ingestion & Config (YÃ¼kleme):
#    - ArÄ±za ve SaÄŸlam verileri yÃ¼klenir.
#    - GÃ¶zlem baÅŸlangÄ±Ã§ tarihi (Left Truncation) veriden otomatik tespit edilir.
#
# 2. ğŸ—ï¸ Dataset Construction (Veri Ä°nÅŸasÄ±):
#    - 'Survival Analysis' formatÄ± kurulur (Duration, Event, Entry).
#    - Kronik arÄ±zalar (Chronic Features) ve zamansal Ã¶zellikler tÃ¼retilir.
#    - Ara dosyalar (Intermediate) kaydedilir.
#
# 3. ğŸ›¡ï¸ Global Modeling (GÃ¼venlik AÄŸÄ± / Fallback):
#    - Verisi az olan (N < 100) ekipman tipleri iÃ§in model eÄŸitilemez.
#    - Bu adÄ±mda tÃ¼m filoyu kullanan "Genel Modeller" (Cox, RSF, ML) eÄŸitilir ve
#      yetersiz verili tipler iÃ§in yedek (fallback) olarak hafÄ±zada tutulur.
#
# 4. âš™ï¸ Stratified Training (KatmanlÄ± EÄŸitim):
#    - Her ekipman tipi iÃ§in dÃ¶ngÃ¼ye girilir.
#    - KARAR MEKANÄ°ZMASI:
#      a. Yeterli Veri Var mÄ±? -> O tipe Ã–ZEL model eÄŸit (En yÃ¼ksek hassasiyet).
#      b. Veri Yetersiz mi? -> GLOBAL modelleri devreye sok (Tahminsiz bÄ±rakma).
#    - Marka ve BakÄ±m analizleri (varsa) bu aÅŸamada Ã¼retilir.
#
# 5. ğŸ¥ Final Scoring (Puanlama & Raporlama):
#    - TÃ¼m tahminler birleÅŸtirilir.
#    - OlasÄ±lÄ±klar (PoF) -> SaÄŸlÄ±k Skoru (0-100) ve Risk SÄ±nÄ±fÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r.
#    - 'pof_predictions_final.csv', 'marka_analysis.csv' vb. master dosyalar kaydedilir.
#
# 6. ğŸ•°ï¸ Backtesting (Zaman TÃ¼neli Testi):
#    - Modelin baÅŸarÄ±sÄ±nÄ± kanÄ±tlamak iÃ§in 2022-2024 yÄ±llarÄ± simÃ¼le edilir.
#    - YÃ¶netim sunumu iÃ§in AUC ve Top-100 isabet oranlarÄ± hesaplanÄ±r.
# =============================================================================

def main():
    ensure_dirs()
    logger = setup_logger()
    # -------------------------------------------------------------------------
    # STEP 1: LOAD & CONFIGURE
    # -------------------------------------------------------------------------
    logger.info("[STEP 1] Loading data...")
    df_fault = load_fault_data(logger)
    df_healthy = load_healthy_data(logger)  
    # Auto-detect start date from data
    # This fixes the Left Truncation logic dynamically
    observation_start_date = df_fault["started at"].min()
    data_end_date = df_fault["started at"].max()
    logger.info(f"[CONFIG] Data range: {observation_start_date.date()} â†’ {data_end_date.date()}")
    logger.info(f"[CONFIG] Observation Start (Left Truncation): {observation_start_date.date()}")
    # -------------------------------------------------------------------------
    # STEP 2: BUILD DATASET
    # -------------------------------------------------------------------------
    logger.info("\n" + "="*60)
    logger.info("PRODUCTION - Training on Full History")
    logger.info("="*60 + "\n")
    logger.info("[STEP 2] Building complete dataset...")
    # 1. Master list
    equipment_master = build_equipment_master(df_fault, df_healthy, logger, data_end_date)

    # 2. Data quality already checked in load_fault_data()
    df_fault_filtered = df_fault.copy()

    # 3. Add Survival Columns (Events, Duration, Delayed Entry)
    df_all = add_survival_columns_inplace(
        equipment_master.copy(),
        df_fault_filtered,
        data_end_date,
        observation_start_date,
        logger
    )
    # Save survival base intermediate
    df_all.to_csv(INTERMEDIATE_PATHS["survival_base"], index=False, encoding="utf-8-sig")
    logger.info(f"[SAVE] Intermediate: {INTERMEDIATE_PATHS['survival_base']}")
    
    # 4. Feature Engineering
    logger.info("[STEP 3] Engineering features...")
    chronic_df = compute_chronic_features(df_fault, data_end_date, logger)
    df_all = add_temporal_features_inplace(
        df_all,
        data_end_date,
        chronic_df,
        observation_start_date,
        logger
    )
    # Define Feature Columns
    structural_cols = ["Ekipman_Tipi", "Gerilim_Sinifi", "Gerilim_Seviyesi", "Marka"]
    structural_cols = [c for c in structural_cols if c in df_all.columns]
    
    temporal_cols = ["Tref_Yas_Gun", "Tref_Ay", "Ariza_Sayisi_90g",
                     "Chronic_Rate_Yillik", "Chronic_Decay_Skoru", "Chronic_Flag",
                     "Observation_Ratio"]
    temporal_cols = [c for c in temporal_cols if c in df_all.columns]

    # Save feature outputs
    if structural_cols:
        df_all[["cbs_id"] + structural_cols].to_csv(INTERMEDIATE_PATHS["features_structural"], index=False, encoding="utf-8-sig")
        logger.info(f"[SAVE] Intermediate: {INTERMEDIATE_PATHS['features_structural']}")
    if temporal_cols:
        df_all[["cbs_id"] + temporal_cols].to_csv(INTERMEDIATE_PATHS["features_temporal"], index=False, encoding="utf-8-sig")
        logger.info(f"[SAVE] Intermediate: {INTERMEDIATE_PATHS['features_temporal']}")
        
    # Save combined feature set (ozellikler_pof)
    all_feature_cols = ["cbs_id"] + structural_cols + temporal_cols + ["event", "duration_days", "entry_days"]
    all_feature_cols = [c for c in all_feature_cols if c in df_all.columns]
    df_all[all_feature_cols].to_csv(INTERMEDIATE_PATHS["ozellikler_pof"], index=False, encoding="utf-8-sig")
    
    logger.info(f"[SAVE] Intermediate: {INTERMEDIATE_PATHS['ozellikler_pof']}")
    logger.info(f"[DATASET] Assets: {len(df_all)} | Features: {len(structural_cols) + len(temporal_cols)}")

    # -------------------------------------------------------------------------
    # STEP 3: TRAIN GLOBAL MODELS (FALLBACK)
    # -------------------------------------------------------------------------
    logger.info("\n[GLOBAL] Training fallback models (Cox, RSF, ML)...")
    
    # 1. Global Cox
    X_cox_global = select_cox_safe_features(df_all, structural_cols, logger)
    cox_global, wb_global = train_cox_weibull(
        X_cox_global,
        df_all["duration_days"],
        df_all["event"],
        df_all["entry_days"],
        logger
    )
    # 2. Global Random Survival Forest
    rsf_global = train_rsf_survival(df_all, structural_cols, logger)  
    # 3. Global ML (Gradient Boosting Survival)
    ml_features_global = structural_cols + [c for c in temporal_cols if c not in ["Kurulum_Tarihi"]]

    ml_pack_global = train_ml_models(df_all, ml_features_global, SURVIVAL_HORIZONS_DAYS, logger)
    # Store global models for fallback usage
    global_models = {
        "cox": cox_global,
        "weibull": wb_global,
        "rsf": rsf_global,
        "ml": ml_pack_global,
        "X_cox_cols": X_cox_global.columns.tolist()
    }
    # -------------------------------------------------------------------------
    # STEP 4: EQUIPMENT-STRATIFIED MODELING
    # -------------------------------------------------------------------------
    logger.info("\n" + "="*60)
    logger.info("STEP 4 - Equipment-Stratified Modeling")
    logger.info("="*60 + "\n")
    eq_stats = get_equipment_stats(df_all, equipment_master, logger)
    unique_types = sorted(df_all["Ekipman_Tipi"].unique())
    
    MIN_SAMPLES = 100
    MIN_EVENTS = 30

    all_predictions = []
    all_marka_analyses = []
    all_bakim_analyses = []
    
    # Import TQDM for progress bar
    from tqdm import tqdm
    for eq_type in tqdm(unique_types, desc="Training Equipment Models", unit="type"):
        try:
            # 1. Filter Data
            df_eq = df_all[df_all["Ekipman_Tipi"] == eq_type].copy()
            stats = eq_stats.get(eq_type, {'n_total': 0, 'n_events': 0, 'has_marka': 0, 'has_bakim': 0})
            preds = pd.DataFrame({"cbs_id": df_eq["cbs_id"]})
            model_source = "Equipment_Specific"
            # 2. DECISION: Use Global Fallback vs Specific Training
            if stats["n_total"] < MIN_SAMPLES or stats["n_events"] < MIN_EVENTS:
                # --- GLOBAL FALLBACK (ENHANCED) ---
                model_source = "Global_Fallback"
                # A) Global Cox Fallback
                try:
                    X_eq = select_cox_safe_features(df_eq, structural_cols, logger)
                    # Align features with global model
                    for c in set(global_models["X_cox_cols"]) - set(X_eq.columns):
                        X_eq[c] = 0
                    X_eq = X_eq[global_models["X_cox_cols"]]

                    if cox_global:
                        cox_pred = predict_survival_pof(cox_global, X_eq, df_eq["duration_days"],
                                                        SURVIVAL_HORIZONS_DAYS, "cox", df_eq["cbs_id"])
                        preds = preds.merge(cox_pred, on="cbs_id", how="left")
                except Exception:
                    pass
                # B) Global RSF Fallback
                try:
                    if rsf_global:
                        rsf_pred = predict_rsf_pof(df_eq, rsf_global, structural_cols, SURVIVAL_HORIZONS_DAYS)
                        preds = preds.merge(rsf_pred, on="cbs_id", how="left")
                except Exception:
                    pass
                # C) Global ML Fallback
                try:
                    if ml_pack_global:
                        # Note: predict_ml_pof should handle missing columns internally
                        ml_pred = predict_ml_pof(df_eq, ml_pack_global, SURVIVAL_HORIZONS_DAYS)
                        preds = preds.merge(ml_pred, on="cbs_id", how="left")
                except Exception:
                    pass
            else:
                # --- SPECIFIC TRAINING ---
                preds = train_equipment_specific_models(df_eq, structural_cols, temporal_cols, eq_type, logger)
                # Specific Explanatory Analyses
                if stats.get("has_marka", 0) >= 30:
                    try:
                        marka_analysis = analyze_marka_effect(df_eq, eq_type, logger)

                        if not marka_analysis.empty: all_marka_analyses.append(marka_analysis)

                    except Exception: pass
                # --- BAKIM ANALÄ°ZÄ° (KoÅŸullu) ---
                if stats.get("has_bakim", 0) >= 30:  # âœ… KOÅUL EKLENDÄ°!
                    try:
                        bakim_analysis = analyze_bakim_effect(df_eq, eq_type, logger)
                        if not bakim_analysis.empty:
                            all_bakim_analyses.append(bakim_analysis)
                    except Exception as e:
                        logger.warning(f"[{eq_type}] Bakim analysis failed: {e}")

            # 3. MERGE PREDICTIONS WITH METADATA (FIXED)
            # We merge 'preds' (which only has cbs_id + probabilities) back to df_eq metadata
            meta_cols = ["cbs_id", "Ekipman_Tipi"]
            if "Fault_Count" in df_eq.columns: meta_cols.append("Fault_Count")
            preds_full = df_eq[meta_cols].merge(preds, on="cbs_id", how="left")
            preds_full["Model_Type"] = model_source

            # 4. COMPUTE HEALTH SCORE
            # Now preds_full definitely has "Ekipman_Tipi", so grouping works
            try:
                preds_full = compute_health_score(preds_full, logger)
            except Exception as e:
                logger.error(f"[{eq_type}] Health score calc failed: {e}")
                preds_full["Health_Score"] = 50
                preds_full["Risk_Sinifi"] = "ORTA"

            # 5. Store Results
            all_predictions.append(preds_full)
            # Save individual CSV (Silent to keep progress bar clean)
            safe_name = str(eq_type).replace("/", "_").replace(" ", "_")
            out_path = os.path.join(OUTPUT_DIR, f"pof_{safe_name}.csv")
            preds_full.to_csv(out_path, index=False, encoding="utf-8-sig")
        except Exception as e:
            logger.error(f"[{eq_type}] Failed to process equipment type: {e}")
            import traceback
            logger.error(traceback.format_exc())
            continue

    # -------------------------------------------------------------------------
    # STEP 5: FINALIZE
    # -------------------------------------------------------------------------
    logger.info("\n" + "="*60)
    logger.info("STEP 5 - Final Ensemble & Reporting")
    logger.info("="*60 + "\n")
    if not all_predictions:
        logger.error("No predictions generated.")
        return

    # Combine all
    predictions = pd.concat(all_predictions, ignore_index=True)

    # âœ… CREATE ENSEMBLE (Average of available models)
    logger.info("[ENSEMBLE] Creating ensemble predictions from available models...")

    for horizon_label in SURVIVAL_HORIZON_LABELS.values():
        # Find all model predictions for this horizon
        model_cols = [c for c in predictions.columns if f"pof_{horizon_label}" in c]

        if model_cols:
            # Average across available models (ignoring NaNs)
            predictions[f"PoF_Ensemble_{horizon_label}"] = predictions[model_cols].mean(axis=1, skipna=True)
            logger.info(f"  - {horizon_label}: Averaged {len(model_cols)} models â†’ PoF_Ensemble_{horizon_label}")
        else:
            # No predictions for this horizon (shouldn't happen but safety check)
            predictions[f"PoF_Ensemble_{horizon_label}"] = 0.0
            logger.warning(f"  - {horizon_label}: No model predictions found, using 0.0")

    # Final Report Merge (Add context like Voltage, Install Date)
    report_cols = ["Ekipman_Tipi", "Gerilim_Sinifi", "Fault_Count", "Kurulum_Tarihi", "Marka", "Ilce"]
    report_base = df_all[["cbs_id"] + [c for c in report_cols if c in df_all.columns]].drop_duplicates("cbs_id")
    # Clean duplicates before merge
    cols_to_drop = [c for c in report_cols if c in predictions.columns]
    preds_clean = predictions.drop(columns=cols_to_drop, errors="ignore")
    report = report_base.merge(preds_clean, on="cbs_id", how="left")
    # Save outputs
    out_path = os.path.join(OUTPUT_DIR, "pof_predictions_final.csv")
    report.to_csv(out_path, index=False, encoding="utf-8-sig")
    logger.info(f"[OUTPUT] Main predictions: {out_path}")
    
    if all_marka_analyses:
        pd.concat(all_marka_analyses).to_csv(os.path.join(OUTPUT_DIR, "marka_analysis.csv"), index=False, encoding="utf-8-sig")

    if all_bakim_analyses:

        pd.concat(all_bakim_analyses).to_csv(os.path.join(OUTPUT_DIR, "bakim_analysis.csv"), index=False, encoding="utf-8-sig")

    # Save intermediate files for Reporting Script (to INTERMEDIATE_DIR not OUTPUT_DIR)
    equipment_master.to_csv(INTERMEDIATE_PATHS["equipment_master"], index=False, encoding="utf-8-sig")
    df_all.to_csv(os.path.join(INTERMEDIATE_DIR, "model_input_data_full.csv"), index=False, encoding="utf-8-sig")
    
    # Final Stats
    critical = (report["Health_Score"] < 20).sum()
    mean_health = report["Health_Score"].mean()

    logger.info(f"Total assets: {len(report):,}")
    logger.info(f"Critical assets (Health<20): {critical:,} ({100*critical/len(report):.1f}%)")
    logger.info(f"Mean Health Score: {mean_health:.1f}")

    # ğŸ”¥ TOP 10 KRÄ°TÄ°K EKiPMAN - SAHA Ã–NCELÄ°ÄÄ°
    if 'PoF_Ensemble_12ay' in report.columns:
        top10_cols = ['cbs_id', 'Ekipman_Tipi', 'PoF_Ensemble_12ay', 'Health_Score', 'Risk_Sinifi']
        # Add optional columns if they exist
        if 'Ilce' in report.columns:
            top10_cols.insert(2, 'Ilce')

        available_cols = [c for c in top10_cols if c in report.columns]
        top10_risk = report.nlargest(10, 'PoF_Ensemble_12ay')[available_cols]

        logger.info("\nğŸš¨ TOP 10 KRÄ°TÄ°K EKiPMAN:")
        logger.info(top10_risk.round(3).to_string(index=False))
    else:
        logger.warning("[TOP 10] PoF_Ensemble_12ay column not found, skipping top 10 report")


    # -------------------------------------------------------------------------
    # STEP 6: BACKTESTING (Temporal Validation)
    # -------------------------------------------------------------------------
    logger.info("\n" + "="*60)
    logger.info("STEP 6 - Temporal Backtesting (Optional)")
    logger.info("="*60 + "\n")
   
    try:
        backtester = TemporalBacktester(df_fault, df_healthy, logger)       
        # Run walk-forward validation for available years
        backtest_results = backtester.run(
            start_year=2022,  # First year to test
            end_year=2024,    # Last year to test
            horizon_days=365  # 12-month prediction window
        )      
        if not backtest_results.empty:

            logger.info("\n[BACKTEST SUMMARY]")
            logger.info(f"Average AUC: {backtest_results['AUC'].mean():.3f}")
            logger.info(f"Average Top-100 Hit Rate: {backtest_results['Top100_Hits'].mean():.1f}")
            logger.info(f"Total Years Tested: {len(backtest_results)}")
        else:
            logger.warning("[BACKTEST] No results generated (insufficient data)")
    except Exception as e:
        logger.error(f"[BACKTEST] Failed: {e}")
        logger.info("[BACKTEST] Skipping temporal validation, continuing with main pipeline")  
    logger.info("="*60)
    logger.info("PIPELINE COMPLETE")

if __name__ == "__main__":
    main()
